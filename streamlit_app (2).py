import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from urllib.parse import urljoin, urlparse
import re

def scrape_data(url, css_selector="a.m-mainlist-item__ttl"):
    """
    æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹

    Args:
        url (str): ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®URL
        css_selector (str): ä½¿ç”¨ã™ã‚‹CSSã‚»ãƒ¬ã‚¯ã‚¿

    Returns:
        tuple: (ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ, ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã™ã‚‹ã‹ã®ãƒ•ãƒ©ã‚°)
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        response = requests.get(url, headers=headers, timeout=10)

        # 404ã‚¨ãƒ©ãƒ¼ã‚„ä»–ã®HTTPã‚¨ãƒ©ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
        if response.status_code == 404:
            return [], False

        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # æŒ‡å®šã•ã‚ŒãŸCSSã‚»ãƒ¬ã‚¯ã‚¿ã§è¦ç´ ã‚’æ¤œç´¢
        elements = soup.select(css_selector)

        # è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºãƒšãƒ¼ã‚¸ã¨åˆ¤å®š
        if not elements:
            return [], False

        data = []
        for element in elements:
            # ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒªãƒ³ã‚¯ã‚’å–å¾—
            text = element.get_text(strip=True)
            link = element.get('href', '')

            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
            if link and not link.startswith('http'):
                link = urljoin(url, link)

            if text:  # ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ãªã„å ´åˆã®ã¿è¿½åŠ 
                data.append({
                    'ã‚¿ã‚¤ãƒˆãƒ«': text,
                    'URL': link,
                    'ã‚½ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸': url
                })

        return data, True

    except requests.exceptions.RequestException as e:
        st.error(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return [], False
    except Exception as e:
        st.error(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        return [], False

def generate_walkerplus_url(base_url, page_num):
    """
    WalkerPlusã®URLæ§‹é€ ã«å¯¾å¿œã—ãŸãƒšãƒ¼ã‚¸URLã‚’ç”Ÿæˆã™ã‚‹

    WalkerPlusã®URLæ§‹é€ :
    - 1ãƒšãƒ¼ã‚¸ç›®: https://walker.plus/example/ (ç•ªå·ãªã—)
    - 2ãƒšãƒ¼ã‚¸ç›®: https://walker.plus/example/2.html
    - 3ãƒšãƒ¼ã‚¸ç›®: https://walker.plus/example/3.html

    Args:
        base_url (str): ãƒ™ãƒ¼ã‚¹URL
        page_num (int): ãƒšãƒ¼ã‚¸ç•ªå·

    Returns:
        str: ç”Ÿæˆã•ã‚ŒãŸãƒšãƒ¼ã‚¸URL
    """
    try:
        if page_num == 1:
            # 1ãƒšãƒ¼ã‚¸ç›®ã¯ãƒ™ãƒ¼ã‚¹URLã‚’ãã®ã¾ã¾ä½¿ç”¨
            return base_url

        # ãƒ™ãƒ¼ã‚¹URLã®æœ«å°¾å‡¦ç†
        if base_url.endswith('/'):
            base_url = base_url.rstrip('/')

        # 2ãƒšãƒ¼ã‚¸ç›®ä»¥é™: ãƒ™ãƒ¼ã‚¹URL + ãƒšãƒ¼ã‚¸ç•ªå·.html
        page_url = f"{base_url}/{page_num}.html"

        return page_url

    except Exception as e:
        st.error(f"WalkerPlus URLç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return base_url

def main():
    st.title("ğŸ” WalkerPluså°‚ç”¨ Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«")
    st.markdown("---")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®š
    st.sidebar.header("âš™ï¸ è¨­å®š")

    # URLå…¥åŠ›
    url = st.sidebar.text_input(
        "ğŸ“ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡URL",
        placeholder="https://walker.plus/example/",
        help="WalkerPlusã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆæœ«å°¾ã¯/ã§çµ‚ã‚ã‚‹ã“ã¨ã‚’æ¨å¥¨ï¼‰"
    )

    # CSSã‚»ãƒ¬ã‚¯ã‚¿å…¥åŠ›ï¼ˆè¦ä»¶1: å¾©æ´»ï¼‰
    css_selector = st.sidebar.text_input(
        "ğŸ¯ CSSã‚»ãƒ¬ã‚¯ã‚¿",
        value="a.m-mainlist-item__ttl",
        help="æŠ½å‡ºã—ãŸã„è¦ç´ ã®CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚WalkerPlusã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: a.m-mainlist-item__ttl"
    )

    # ãƒšãƒ¼ã‚¸æ•°é¸æŠï¼ˆè¦ä»¶2: number_inputã«å¤‰æ›´ã€ä¸Šé™ãªã—ï¼‰
    max_pages = st.sidebar.number_input(
        "ğŸ“„ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒšãƒ¼ã‚¸æ•°",
        min_value=1,
        value=5,
        step=1,
        help="ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆä¸Šé™ãªã—ï¼‰"
    )

    # å¾…æ©Ÿæ™‚é–“è¨­å®š
    wait_time = st.sidebar.slider(
        "â±ï¸ ãƒšãƒ¼ã‚¸é–“å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰",
        min_value=1,
        max_value=10,
        value=3,
        help="å„ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–“éš”ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼ˆWalkerPlusã‚µãƒ¼ãƒãƒ¼ã¸ã®è² è·è»½æ¸›ï¼‰"
    )

    st.sidebar.markdown("---")

    # WalkerPlus URLæ§‹é€ ã®èª¬æ˜
    with st.sidebar.expander("ğŸŒ WalkerPlus URLæ§‹é€ "):
        st.markdown("""
        **WalkerPlusã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³:**
        - 1ãƒšãƒ¼ã‚¸ç›®: `https://walker.plus/example/`
        - 2ãƒšãƒ¼ã‚¸ç›®: `https://walker.plus/example/2.html`
        - 3ãƒšãƒ¼ã‚¸ç›®: `https://walker.plus/example/3.html`

        ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ä¸Šè¨˜ã®æ§‹é€ ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚
        """)

    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ©Ÿèƒ½ï¼ˆç¶­æŒï¼‰
    if url and st.sidebar.button("ğŸ‘€ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆ1ãƒšãƒ¼ã‚¸ç›®ã®ã¿ï¼‰"):
        with st.spinner("ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å–å¾—ä¸­..."):
            preview_data, page_exists = scrape_data(url, css_selector)

            if not page_exists:
                st.error("âŒ ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã—ãªã„ã‹ã€æŒ‡å®šã•ã‚ŒãŸCSSã‚»ãƒ¬ã‚¯ã‚¿ã§è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.info("ğŸ’¡ URLã‚„CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            elif preview_data:
                st.success(f"âœ… {len(preview_data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼")

                # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
                st.subheader("ğŸ“‹ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿")
                preview_df = pd.DataFrame(preview_data)
                st.dataframe(preview_df, use_container_width=True)

                # ä½¿ç”¨ã—ãŸCSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’è¡¨ç¤º
                st.info(f"ğŸ¯ ä½¿ç”¨ã—ãŸCSSã‚»ãƒ¬ã‚¯ã‚¿: `{css_selector}`")
            else:
                st.warning("âš ï¸ æŒ‡å®šã•ã‚ŒãŸCSSã‚»ãƒ¬ã‚¯ã‚¿ã§ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚»ãƒ¬ã‚¯ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    st.sidebar.markdown("---")

    # ãƒ¡ã‚¤ãƒ³ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    if url and st.sidebar.button("ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹", type="primary"):
        if not css_selector.strip():
            st.error("âŒ CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        # çµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
        all_data = []

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
        progress_bar = st.progress(0)
        status_text = st.empty()
        results_container = st.empty()

        # è‡ªå‹•çµ‚äº†ãƒ•ãƒ©ã‚°ï¼ˆè¦ä»¶3: è‡ªå‹•çµ‚äº†æ©Ÿèƒ½ï¼‰
        auto_terminated = False
        actual_pages_scraped = 0

        try:
            for page in range(1, max_pages + 1):
                # WalkerPluså°‚ç”¨ã®ãƒšãƒ¼ã‚¸URLã‚’ç”Ÿæˆ
                page_url = generate_walkerplus_url(url, page)

                status_text.text(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page}/{max_pages} ã‚’å‡¦ç†ä¸­... ({page_url})")

                # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆè¦ä»¶4: css_selectorãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰
                page_data, page_exists = scrape_data(page_url, css_selector)

                # ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯è‡ªå‹•çµ‚äº†ï¼ˆè¦ä»¶3: è‡ªå‹•çµ‚äº†æ©Ÿèƒ½ï¼‰
                if not page_exists:
                    auto_terminated = True
                    actual_pages_scraped = page - 1
                    break

                # ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                if page_data:
                    all_data.extend(page_data)
                    actual_pages_scraped = page

                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’æ›´æ–°
                progress_bar.progress(page / max_pages)

                # ä¸­é–“çµæœã‚’è¡¨ç¤º
                if all_data:
                    with results_container.container():
                        st.subheader(f"ğŸ“Š ç¾åœ¨ã®çµæœ ({len(all_data)}ä»¶)")
                        temp_df = pd.DataFrame(all_data)
                        st.dataframe(temp_df.tail(10), use_container_width=True)

                # æœ€å¾Œã®ãƒšãƒ¼ã‚¸ã§ãªã„å ´åˆã¯å¾…æ©Ÿï¼ˆãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿæ™‚é–“ç¶­æŒï¼‰
                if page < max_pages:
                    wait_seconds = random.uniform(wait_time, wait_time + 2)
                    time.sleep(wait_seconds)

            # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            progress_bar.progress(1.0)

            # è¦ä»¶3: è‡ªå‹•çµ‚äº†æ™‚ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
            if auto_terminated:
                status_text.success(f"âœ… è‡ªå‹•çµ‚äº†: æŒ‡å®šãƒšãƒ¼ã‚¸æ•°({max_pages})ã«é”ã™ã‚‹å‰ã«çµ‚äº†ã—ã¾ã—ãŸï¼ˆå®Ÿéš›: {actual_pages_scraped}ãƒšãƒ¼ã‚¸ï¼‰")
                st.info(f"â„¹ï¸ ãƒšãƒ¼ã‚¸ {actual_pages_scraped + 1} ãŒå­˜åœ¨ã—ãªã„ã‹ã€ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚è‡ªå‹•ã§çµ‚äº†ã—ã¾ã—ãŸã€‚")
            else:
                status_text.success(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†! {max_pages}ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†ã—ã¾ã—ãŸã€‚")

            # æœ€çµ‚çµæœã®è¡¨ç¤º
            if all_data:
                st.subheader(f"ğŸ“ˆ æœ€çµ‚çµæœ ({len(all_data)}ä»¶)")

                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
                df = pd.DataFrame(all_data)

                # çµæœã‚’è¡¨ç¤º
                st.dataframe(df, use_container_width=True)

                # çµ±è¨ˆæƒ…å ±
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ğŸ“Š ç·ãƒ‡ãƒ¼ã‚¿æ•°", len(all_data))
                with col2:
                    st.metric("ğŸ“„ å‡¦ç†ãƒšãƒ¼ã‚¸æ•°", actual_pages_scraped)
                with col3:
                    unique_urls = df['URL'].nunique() if 'URL' in df.columns else 0
                    st.metric("ğŸ”— ãƒ¦ãƒ‹ãƒ¼ã‚¯URLæ•°", unique_urls)

                # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆç¶­æŒï¼‰
                csv = df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ğŸ“¥ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv,
                    file_name=f"walkerplus_scraped_data_{int(time.time())}.csv",
                    mime="text/csv"
                )

                # ä½¿ç”¨ã—ãŸè¨­å®šæƒ…å ±ã‚’è¡¨ç¤º
                st.subheader("âš™ï¸ ä½¿ç”¨ã—ãŸè¨­å®š")
                settings_df = pd.DataFrame({
                    'è¨­å®šé …ç›®': ['ãƒ™ãƒ¼ã‚¹URL', 'CSSã‚»ãƒ¬ã‚¯ã‚¿', 'æŒ‡å®šãƒšãƒ¼ã‚¸æ•°', 'å®Ÿéš›ã®å‡¦ç†ãƒšãƒ¼ã‚¸æ•°', 'å¾…æ©Ÿæ™‚é–“', 'URLæ§‹é€ '],
                    'å€¤': [url, css_selector, max_pages, actual_pages_scraped, f"{wait_time}ç§’", 'WalkerPluså°‚ç”¨']
                })
                st.table(settings_df)

                # ç”Ÿæˆã•ã‚ŒãŸURLã®ç¢ºèª
                st.subheader("ğŸ”— ç”Ÿæˆã•ã‚ŒãŸURLä¸€è¦§")
                url_list = []
                for i in range(1, actual_pages_scraped + 1):
                    generated_url = generate_walkerplus_url(url, i)
                    url_list.append({
                        'ãƒšãƒ¼ã‚¸': i,
                        'ç”ŸæˆURL': generated_url
                    })

                if url_list:
                    url_df = pd.DataFrame(url_list)
                    st.dataframe(url_df, use_container_width=True)

            else:
                st.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚URLã‚„CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        except Exception as e:
            st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            status_text.error("âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")

    # ä½¿ç”¨æ–¹æ³•ã®èª¬æ˜ï¼ˆè¦ä»¶5: UIæ”¹å–„ï¼‰
    with st.expander("ğŸ“– ä½¿ç”¨æ–¹æ³•"):
        st.markdown("""
        ### ğŸ”§ åŸºæœ¬çš„ãªä½¿ã„æ–¹
        1. **URLå…¥åŠ›**: WalkerPlusã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡URLã‚’å…¥åŠ›
        2. **CSSã‚»ãƒ¬ã‚¯ã‚¿æŒ‡å®š**: æŠ½å‡ºã—ãŸã„è¦ç´ ã®CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’å…¥åŠ›
        3. **ãƒšãƒ¼ã‚¸æ•°è¨­å®š**: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°ã‚’æŒ‡å®šï¼ˆä¸Šé™ãªã—ï¼‰
        4. **ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**: ã¾ãšã€Œãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€ã§1ãƒšãƒ¼ã‚¸ç›®ã®çµæœã‚’ç¢ºèª
        5. **å®Ÿè¡Œ**: ã€Œã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ã€ã§ãƒ‡ãƒ¼ã‚¿åé›†ã‚’é–‹å§‹

        ### ğŸ¯ WalkerPlusç”¨CSSã‚»ãƒ¬ã‚¯ã‚¿ã®ä¾‹
        - `a.m-mainlist-item__ttl` - WalkerPlusã®ã‚¿ã‚¤ãƒˆãƒ«ãƒªãƒ³ã‚¯ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        - `a.m-mainlist-item__link` - ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆã®ãƒªãƒ³ã‚¯
        - `.m-mainlist-item__ttl` - ã‚¿ã‚¤ãƒˆãƒ«è¦ç´ 
        - `.m-mainlist-item` - ãƒªã‚¹ãƒˆã‚¢ã‚¤ãƒ†ãƒ å…¨ä½“

        ### ğŸŒ WalkerPlus URLæ§‹é€ å¯¾å¿œ
        ã“ã®ãƒ„ãƒ¼ãƒ«ã¯WalkerPlusã®ç‰¹æ®ŠãªURLæ§‹é€ ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ï¼š

        **URLç”Ÿæˆãƒ‘ã‚¿ãƒ¼ãƒ³:**
        - **1ãƒšãƒ¼ã‚¸ç›®**: `https://walker.plus/example/` ï¼ˆç•ªå·ãªã—ï¼‰
        - **2ãƒšãƒ¼ã‚¸ç›®**: `https://walker.plus/example/2.html`
        - **3ãƒšãƒ¼ã‚¸ç›®**: `https://walker.plus/example/3.html`
        - **4ãƒšãƒ¼ã‚¸ç›®**: `https://walker.plus/example/4.html`

        **å…¥åŠ›ä¾‹:**
        - ãƒ™ãƒ¼ã‚¹URL: `https://walker.plus/tochigi/gourmet/`
        - ç”Ÿæˆã•ã‚Œã‚‹2ãƒšãƒ¼ã‚¸ç›®: `https://walker.plus/tochigi/gourmet/2.html`

        ### ğŸ”„ è‡ªå‹•çµ‚äº†æ©Ÿèƒ½
        - æŒ‡å®šã—ãŸãƒšãƒ¼ã‚¸æ•°ã‚ˆã‚Šå®Ÿéš›ã®ãƒšãƒ¼ã‚¸ãŒå°‘ãªã„å ´åˆã€è‡ªå‹•ã§çµ‚äº†
        - 404ã‚¨ãƒ©ãƒ¼ã‚„ç©ºãƒšãƒ¼ã‚¸ã‚’æ¤œå‡ºã—ã¦åœæ­¢
        - ç„¡é§„ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é˜²ãã€åŠ¹ç‡çš„ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        - ã€ŒæŒ‡å®šãƒšãƒ¼ã‚¸æ•°ã«é”ã™ã‚‹å‰ã«çµ‚äº†ã—ã¾ã—ãŸã€ã¨ã„ã†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º

        ### ğŸ›¡ï¸ å®‰å…¨æ©Ÿèƒ½
        - **ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿæ™‚é–“**: è¨­å®šæ™‚é–“+0ã€œ2ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ é–“éš”ã§ã‚¢ã‚¯ã‚»ã‚¹
        - **User-Agentè¨­å®š**: ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’æ¨¡æ“¬
        - **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: å„ç¨®ã‚¨ãƒ©ãƒ¼ã«å¯¾ã™ã‚‹é©åˆ‡ãªå‡¦ç†
        - **ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š**: 10ç§’ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

        ### âš ï¸ æ³¨æ„äº‹é …
        - WalkerPlusã®åˆ©ç”¨è¦ç´„ã‚’éµå®ˆã—ã¦ãã ã•ã„
        - éåº¦ãªã‚¢ã‚¯ã‚»ã‚¹ã¯é¿ã‘ã¦ãã ã•ã„
        - robots.txtã‚’ç¢ºèªã—ã¦ãã ã•ã„
        - ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†é©åˆ‡ãªé–“éš”ã‚’è¨­å®šã—ã¦ãã ã•ã„
        - ã“ã®ãƒ„ãƒ¼ãƒ«ã¯WalkerPluså°‚ç”¨ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™
        """)

if __name__ == "__main__":
    main()
