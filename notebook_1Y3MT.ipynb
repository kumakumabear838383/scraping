{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Streamlitã‚’ä½¿ã£ãŸå®‰å…¨ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®è¦ä»¶ã‚’æº€ãŸã™ã‚¢ãƒ—ãƒªã‚’ä½œæˆï¼š\n\n1. **ãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½**ï¼š\n   - ãƒ™ãƒ¼ã‚¹URLå…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰\n   - CSSã‚»ãƒ¬ã‚¯ã‚¿å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰  \n   - ãƒšãƒ¼ã‚¸ç¯„å›²è¨­å®šï¼ˆé–‹å§‹ãƒšãƒ¼ã‚¸ã€çµ‚äº†ãƒšãƒ¼ã‚¸ï¼‰\n   - ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œãƒœã‚¿ãƒ³\n\n2. **å®‰å…¨æ©Ÿèƒ½**ï¼š\n   - å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã«1-5ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿæ™‚é–“\n   - æœ€ä½1ç§’ã®å¾…æ©Ÿä¿è¨¼\n   - é©åˆ‡ãªUser-Agentè¨­å®š\n   - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°\n\n3. **ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£**ï¼š\n   - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—è¡¨ç¤º\n   - å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º\n   - Excel/CSV/ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½\n\n4. **CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚¬ã‚¤ãƒ‰**ï¼š\n   - å³ã‚¯ãƒªãƒƒã‚¯â†’æ¤œè¨¼â†’Copy selectorã®æ‰‹é †èª¬æ˜\n   - ã‚µãƒ³ãƒ—ãƒ«ã‚»ãƒ¬ã‚¯ã‚¿ã®è¡¨ç¤º\n\n5. **ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š**ï¼š\n   - ãƒ™ãƒ¼ã‚¹URL: \"https://www.walkerplus.com/spot_list/ar0700/sg0107/\"\n   - CSSã‚»ãƒ¬ã‚¯ã‚¿: \"a.m-mainlist-item__ttl\"\n   - ãƒšãƒ¼ã‚¸ç¯„å›²: 1-5ãƒšãƒ¼ã‚¸\n\nä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆï¼š\n- streamlit_app.py (ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒª)\n- requirements.txt (ä¾å­˜é–¢ä¿‚)\n- README.md (ä½¿ç”¨æ–¹æ³•èª¬æ˜)\n\nå®Ÿéš›ã«å‹•ä½œã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Streamlitã‚’ä½¿ã£ãŸå®‰å…¨ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚å®Ÿéš›ã«å‹•ä½œã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¾ã™ã­ï¼"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\nimport streamlit as st\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\nimport random\nfrom urllib.parse import urljoin, urlparse\nimport io\nfrom datetime import datetime\n\n# Create output directory if it doesn't exist\noutput_dir = '/home/user/output'\nos.makedirs(output_dir, exist_ok=True)\n\n# Create streamlit_app.py\nstreamlit_app_code = '''import streamlit as st\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\nimport random\nfrom urllib.parse import urljoin, urlparse\nimport io\nfrom datetime import datetime\n\n# Page configuration\nst.set_page_config(\n    page_title=\"å®‰å…¨ãªWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«\",\n    page_icon=\"ğŸ•·ï¸\",\n    layout=\"wide\"\n)\n\n# Custom CSS\nst.markdown(\"\"\"\n<style>\n.main-header {\n    font-size: 2.5rem;\n    color: #1f77b4;\n    text-align: center;\n    margin-bottom: 2rem;\n}\n.section-header {\n    font-size: 1.5rem;\n    color: #ff7f0e;\n    margin-top: 2rem;\n    margin-bottom: 1rem;\n}\n.info-box {\n    background-color: #f0f2f6;\n    padding: 1rem;\n    border-radius: 0.5rem;\n    border-left: 4px solid #1f77b4;\n    margin: 1rem 0;\n}\n.warning-box {\n    background-color: #fff3cd;\n    padding: 1rem;\n    border-radius: 0.5rem;\n    border-left: 4px solid #ffc107;\n    margin: 1rem 0;\n}\n.success-box {\n    background-color: #d4edda;\n    padding: 1rem;\n    border-radius: 0.5rem;\n    border-left: 4px solid #28a745;\n    margin: 1rem 0;\n}\n</style>\n\"\"\", unsafe_allow_html=True)\n\n# Title\nst.markdown('<h1 class=\"main-header\">ğŸ•·ï¸ å®‰å…¨ãªWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«</h1>', unsafe_allow_html=True)\n\n# Warning message\nst.markdown(\"\"\"\n<div class=\"warning-box\">\n<strong>âš ï¸ é‡è¦ãªæ³¨æ„äº‹é …</strong><br>\nã“ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€å¯¾è±¡ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ã¨robots.txtã‚’å¿…ãšç¢ºèªã—ã¦ãã ã•ã„ã€‚\né©åˆ‡ãªé–“éš”ã§ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’å¿ƒãŒã‘ã€ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n</div>\n\"\"\", unsafe_allow_html=True)\n\n# Sidebar for settings\nst.sidebar.markdown('<h2 class=\"section-header\">âš™ï¸ è¨­å®š</h2>', unsafe_allow_html=True)\n\n# Input fields\nbase_url = st.sidebar.text_input(\n    \"ãƒ™ãƒ¼ã‚¹URL\",\n    value=\"https://www.walkerplus.com/spot_list/ar0700/sg0107/\",\n    help=\"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®ãƒ™ãƒ¼ã‚¹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„\"\n)\n\ncss_selector = st.sidebar.text_input(\n    \"CSSã‚»ãƒ¬ã‚¯ã‚¿\",\n    value=\"a.m-mainlist-item__ttl\",\n    help=\"å–å¾—ã—ãŸã„è¦ç´ ã®CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„\"\n)\n\n# Page range\ncol1, col2 = st.sidebar.columns(2)\nwith col1:\n    start_page = st.number_input(\"é–‹å§‹ãƒšãƒ¼ã‚¸\", min_value=1, value=1)\nwith col2:\n    end_page = st.number_input(\"çµ‚äº†ãƒšãƒ¼ã‚¸\", min_value=1, value=5)\n\n# Advanced settings\nst.sidebar.markdown(\"### è©³ç´°è¨­å®š\")\nmin_delay = st.sidebar.slider(\"æœ€å°å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰\", 1, 10, 1)\nmax_delay = st.sidebar.slider(\"æœ€å¤§å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰\", 1, 10, 5)\ntimeout = st.sidebar.slider(\"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰\", 5, 30, 10)\n\n# CSS Selector Guide\nst.markdown('<h2 class=\"section-header\">ğŸ“– CSSã‚»ãƒ¬ã‚¯ã‚¿ã®å–å¾—æ–¹æ³•</h2>', unsafe_allow_html=True)\n\nwith st.expander(\"CSSã‚»ãƒ¬ã‚¯ã‚¿ã®å–å¾—æ‰‹é †\", expanded=False):\n    st.markdown(\"\"\"\n    ### æ‰‹é †ï¼š\n    1. **å¯¾è±¡ã‚µã‚¤ãƒˆã‚’é–‹ã** - ãƒ–ãƒ©ã‚¦ã‚¶ã§å¯¾è±¡ã®Webãƒšãƒ¼ã‚¸ã‚’é–‹ãã¾ã™\n    2. **è¦ç´ ã‚’å³ã‚¯ãƒªãƒƒã‚¯** - å–å¾—ã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆã‚„ãƒªãƒ³ã‚¯ã‚’å³ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™\n    3. **æ¤œè¨¼ã‚’é¸æŠ** - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ã€Œæ¤œè¨¼ã€ã¾ãŸã¯ã€Œè¦ç´ ã‚’èª¿æŸ»ã€ã‚’é¸æŠ\n    4. **è¦ç´ ã‚’ã‚³ãƒ”ãƒ¼** - é–‹ç™ºè€…ãƒ„ãƒ¼ãƒ«ã§è¦ç´ ã‚’å³ã‚¯ãƒªãƒƒã‚¯ â†’ Copy â†’ Copy selector\n    5. **ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è²¼ã‚Šä»˜ã‘** - ä¸Šè¨˜ã®å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«è²¼ã‚Šä»˜ã‘ã¾ã™\n    \n    ### ã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹ã‚»ãƒ¬ã‚¯ã‚¿ã®ä¾‹ï¼š\n    - `a` - ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯\n    - `.class-name` - ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‚’æŒã¤è¦ç´ \n    - `#id-name` - ç‰¹å®šã®IDã‚’æŒã¤è¦ç´ \n    - `h1, h2, h3` - è¦‹å‡ºã—è¦ç´ \n    - `p` - æ®µè½è¦ç´ \n    - `div.container a` - containerã‚¯ãƒ©ã‚¹å†…ã®ãƒªãƒ³ã‚¯\n    \"\"\")\n\n# Function to scrape data\ndef scrape_data(base_url, css_selector, start_page, end_page, min_delay, max_delay, timeout):\n    \"\"\"\n    Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°\n    \"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n    }\n    \n    results = []\n    progress_bar = st.progress(0)\n    status_text = st.empty()\n    \n    total_pages = end_page - start_page + 1\n    \n    for page_num in range(start_page, end_page + 1):\n        try:\n            # Construct URL\n            if '{}' in base_url:\n                url = base_url.format(page_num)\n            elif base_url.endswith('/'):\n                url = f\"{base_url}?page={page_num}\"\n            else:\n                url = f\"{base_url}&page={page_num}\"\n            \n            status_text.text(f\"ãƒšãƒ¼ã‚¸ {page_num} ã‚’å‡¦ç†ä¸­... URL: {url}\")\n            \n            # Make request\n            response = requests.get(url, headers=headers, timeout=timeout)\n            response.raise_for_status()\n            \n            # Parse HTML\n            soup = BeautifulSoup(response.content, 'html.parser')\n            elements = soup.select(css_selector)\n            \n            # Extract data\n            page_results = []\n            for i, element in enumerate(elements):\n                data = {\n                    'page': page_num,\n                    'index': i + 1,\n                    'text': element.get_text(strip=True),\n                    'href': element.get('href', ''),\n                    'full_url': urljoin(url, element.get('href', '')) if element.get('href') else '',\n                    'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n                }\n                page_results.append(data)\n            \n            results.extend(page_results)\n            \n            # Update progress\n            progress = (page_num - start_page + 1) / total_pages\n            progress_bar.progress(progress)\n            \n            status_text.text(f\"ãƒšãƒ¼ã‚¸ {page_num} å®Œäº† - {len(page_results)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\")\n            \n            # Random delay\n            if page_num < end_page:  # Don't delay after the last page\n                delay = random.uniform(min_delay, max_delay)\n                time.sleep(delay)\n                \n        except requests.exceptions.RequestException as e:\n            st.error(f\"ãƒšãƒ¼ã‚¸ {page_num} ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\")\n            continue\n        except Exception as e:\n            st.error(f\"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ (ãƒšãƒ¼ã‚¸ {page_num}): {str(e)}\")\n            continue\n    \n    progress_bar.progress(1.0)\n    status_text.text(f\"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†! åˆè¨ˆ {len(results)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚\")\n    \n    return results\n\n# Main content area\nst.markdown('<h2 class=\"section-header\">ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ</h2>', unsafe_allow_html=True)\n\n# Validation\nif st.button(\"ğŸ•·ï¸ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹\", type=\"primary\", use_container_width=True):\n    if not base_url:\n        st.error(\"ãƒ™ãƒ¼ã‚¹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\")\n    elif not css_selector:\n        st.error(\"CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\")\n    elif start_page > end_page:\n        st.error(\"é–‹å§‹ãƒšãƒ¼ã‚¸ã¯çµ‚äº†ãƒšãƒ¼ã‚¸ä»¥ä¸‹ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\")\n    else:\n        with st.spinner(\"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­...\"):\n            results = scrape_data(base_url, css_selector, start_page, end_page, min_delay, max_delay, timeout)\n        \n        if results:\n            st.success(f\"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†! {len(results)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚\")\n            \n            # Store results in session state\n            st.session_state.scraping_results = results\n            st.session_state.scraping_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n\n# Display results if available\nif 'scraping_results' in st.session_state and st.session_state.scraping_results:\n    st.markdown('<h2 class=\"section-header\">ğŸ“Š å–å¾—ãƒ‡ãƒ¼ã‚¿</h2>', unsafe_allow_html=True)\n    \n    results = st.session_state.scraping_results\n    df = pd.DataFrame(results)\n    \n    # Data preview\n    st.markdown(\"### ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼\")\n    st.dataframe(df, use_container_width=True)\n    \n    # Statistics\n    col1, col2, col3, col4 = st.columns(4)\n    with col1:\n        st.metric(\"ç·ãƒ‡ãƒ¼ã‚¿æ•°\", len(results))\n    with col2:\n        st.metric(\"ãƒšãƒ¼ã‚¸æ•°\", df['page'].nunique())\n    with col3:\n        st.metric(\"ãƒªãƒ³ã‚¯æ•°\", df[df['href'] != ''].shape[0])\n    with col4:\n        st.metric(\"ãƒ†ã‚­ã‚¹ãƒˆã®ã¿\", df[df['href'] == ''].shape[0])\n    \n    # Download section\n    st.markdown('<h2 class=\"section-header\">ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰</h2>', unsafe_allow_html=True)\n    \n    timestamp = st.session_state.scraping_timestamp\n    \n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        # Excel download\n        excel_buffer = io.BytesIO()\n        df.to_excel(excel_buffer, index=False, engine='openpyxl')\n        excel_buffer.seek(0)\n        \n        st.download_button(\n            label=\"ğŸ“Š Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\",\n            data=excel_buffer.getvalue(),\n            file_name=f\"scraping_results_{timestamp}.xlsx\",\n            mime=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n            use_container_width=True\n        )\n    \n    with col2:\n        # CSV download\n        csv_buffer = io.StringIO()\n        df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')\n        \n        st.download_button(\n            label=\"ğŸ“„ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\",\n            data=csv_buffer.getvalue(),\n            file_name=f\"scraping_results_{timestamp}.csv\",\n            mime=\"text/csv\",\n            use_container_width=True\n        )\n    \n    with col3:\n        # Text download\n        text_content = \"\\\\n\".join([f\"{item['text']} - {item['full_url']}\" for item in results if item['text']])\n        \n        st.download_button(\n            label=\"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\",\n            data=text_content,\n            file_name=f\"scraping_results_{timestamp}.txt\",\n            mime=\"text/plain\",\n            use_container_width=True\n        )\n\n# Footer\nst.markdown(\"---\")\nst.markdown(\"\"\"\n<div class=\"info-box\">\n<strong>ğŸ’¡ ä½¿ç”¨ä¸Šã®æ³¨æ„</strong><br>\nâ€¢ ã“ã®ãƒ„ãƒ¼ãƒ«ã¯æ•™è‚²ãƒ»ç ”ç©¶ç›®çš„ã§ã®ä½¿ç”¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™<br>\nâ€¢ å•†ç”¨åˆ©ç”¨ã®å ´åˆã¯ã€å¯¾è±¡ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ã‚’å¿…ãšç¢ºèªã—ã¦ãã ã•ã„<br>\nâ€¢ éåº¦ãªã‚¢ã‚¯ã‚»ã‚¹ã¯ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™<br>\nâ€¢ robots.txtãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’å°Šé‡ã—ã¦ãã ã•ã„\n</div>\n\"\"\", unsafe_allow_html=True)\n'''\n\n# Create requirements.txt\nrequirements_txt = '''streamlit>=1.28.0\nrequests>=2.31.0\nbeautifulsoup4>=4.12.0\npandas>=2.0.0\nopenpyxl>=3.1.0\nlxml>=4.9.0\n'''\n\n# Create README.md\nreadme_md = '''# å®‰å…¨ãªWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«\n\nã“ã®Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€Webã‚µã‚¤ãƒˆã‹ã‚‰å®‰å…¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n\n## ğŸš€ æ©Ÿèƒ½\n\n- **å®‰å…¨ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°**: 1-5ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿæ™‚é–“ã§ã‚µãƒ¼ãƒãƒ¼è² è·ã‚’è»½æ¸›\n- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—è¡¨ç¤º**: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®é€²è¡ŒçŠ¶æ³ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ç¢ºèª\n- **ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**: å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ãã®å ´ã§ç¢ºèª\n- **å¤šå½¢å¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**: Excelã€CSVã€ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n- **CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚¬ã‚¤ãƒ‰**: åˆå¿ƒè€…å‘ã‘ã®è©³ç´°ãªä½¿ç”¨æ–¹æ³•èª¬æ˜\n- **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: å …ç‰¢ãªã‚¨ãƒ©ãƒ¼å‡¦ç†æ©Ÿèƒ½\n\n## ğŸ“‹ è¦ä»¶\n\n- Python 3.8ä»¥ä¸Š\n- å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯`requirements.txt`ã«è¨˜è¼‰\n\n## ğŸ› ï¸ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n\n1. ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã¾ãŸã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n```bash\ngit clone <repository-url>\ncd web-scraping-tool\n```\n\n2. ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n```bash\npip install -r requirements.txt\n```\n\n3. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•\n```bash\nstreamlit run streamlit_app.py\n```\n\n## ğŸ“– ä½¿ç”¨æ–¹æ³•\n\n### åŸºæœ¬çš„ãªä½¿ã„æ–¹\n\n1. **ãƒ™ãƒ¼ã‚¹URLå…¥åŠ›**: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®Webã‚µã‚¤ãƒˆã®URLã‚’å…¥åŠ›\n2. **CSSã‚»ãƒ¬ã‚¯ã‚¿å…¥åŠ›**: å–å¾—ã—ãŸã„è¦ç´ ã®CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’å…¥åŠ›\n3. **ãƒšãƒ¼ã‚¸ç¯„å›²è¨­å®š**: é–‹å§‹ãƒšãƒ¼ã‚¸ã¨çµ‚äº†ãƒšãƒ¼ã‚¸ã‚’æŒ‡å®š\n4. **ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ**: ã€Œã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯\n\n### CSSã‚»ãƒ¬ã‚¯ã‚¿ã®å–å¾—æ–¹æ³•\n\n1. å¯¾è±¡ã‚µã‚¤ãƒˆã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã\n2. å–å¾—ã—ãŸã„è¦ç´ ã‚’å³ã‚¯ãƒªãƒƒã‚¯\n3. ã€Œæ¤œè¨¼ã€ã¾ãŸã¯ã€Œè¦ç´ ã‚’èª¿æŸ»ã€ã‚’é¸æŠ\n4. é–‹ç™ºè€…ãƒ„ãƒ¼ãƒ«ã§è¦ç´ ã‚’å³ã‚¯ãƒªãƒƒã‚¯ â†’ Copy â†’ Copy selector\n5. å–å¾—ã—ãŸã‚»ãƒ¬ã‚¯ã‚¿ã‚’å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«è²¼ã‚Šä»˜ã‘\n\n### ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š\n\n- **ãƒ™ãƒ¼ã‚¹URL**: `https://www.walkerplus.com/spot_list/ar0700/sg0107/`\n- **CSSã‚»ãƒ¬ã‚¯ã‚¿**: `a.m-mainlist-item__ttl`\n- **ãƒšãƒ¼ã‚¸ç¯„å›²**: 1-5ãƒšãƒ¼ã‚¸\n- **å¾…æ©Ÿæ™‚é–“**: 1-5ç§’ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰\n\n## âš™ï¸ è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³\n\n### åŸºæœ¬è¨­å®š\n- **ãƒ™ãƒ¼ã‚¹URL**: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®URL\n- **CSSã‚»ãƒ¬ã‚¯ã‚¿**: å–å¾—ã™ã‚‹è¦ç´ ã®æŒ‡å®š\n- **ãƒšãƒ¼ã‚¸ç¯„å›²**: é–‹å§‹ãƒ»çµ‚äº†ãƒšãƒ¼ã‚¸ã®æŒ‡å®š\n\n### è©³ç´°è¨­å®š\n- **æœ€å°å¾…æ©Ÿæ™‚é–“**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®æœ€å°å¾…æ©Ÿæ™‚é–“ï¼ˆ1-10ç§’ï¼‰\n- **æœ€å¤§å¾…æ©Ÿæ™‚é–“**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®æœ€å¤§å¾…æ©Ÿæ™‚é–“ï¼ˆ1-10ç§’ï¼‰\n- **ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆ5-30ç§’ï¼‰\n\n## ğŸ“Š å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿\n\nå–å¾—ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã«ã¯ä»¥ä¸‹ã®æƒ…å ±ãŒå«ã¾ã‚Œã¾ã™ï¼š\n\n- **page**: ãƒšãƒ¼ã‚¸ç•ªå·\n- **index**: ãƒšãƒ¼ã‚¸å†…ã§ã®è¦ç´ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n- **text**: è¦ç´ ã®ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹\n- **href**: ãƒªãƒ³ã‚¯ã®hrefå±æ€§ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n- **full_url**: å®Œå…¨ãªURLï¼ˆç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›ï¼‰\n- **scraped_at**: ãƒ‡ãƒ¼ã‚¿å–å¾—æ—¥æ™‚\n\n## ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å½¢å¼\n\n- **Excel (.xlsx)**: è¡¨å½¢å¼ã§ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿\n- **CSV (.csv)**: æ±ç”¨çš„ãªãƒ‡ãƒ¼ã‚¿å½¢å¼\n- **ãƒ†ã‚­ã‚¹ãƒˆ (.txt)**: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆå½¢å¼\n\n## âš ï¸ é‡è¦ãªæ³¨æ„äº‹é …\n\n### æ³•çš„ãƒ»å€«ç†çš„ãªè€ƒæ…®äº‹é …\n\n1. **åˆ©ç”¨è¦ç´„ã®ç¢ºèª**: å¯¾è±¡ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ã‚’å¿…ãšç¢ºèªã—ã¦ãã ã•ã„\n2. **robots.txtã®å°Šé‡**: ã‚µã‚¤ãƒˆã®robots.txtãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¢ºèªã—ã€å°Šé‡ã—ã¦ãã ã•ã„\n3. **é©åˆ‡ãªé–“éš”**: ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†ã€é©åˆ‡ãªé–“éš”ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãã ã•ã„\n4. **å•†ç”¨åˆ©ç”¨**: å•†ç”¨ç›®çš„ã§ã®ä½¿ç”¨ã¯ã€äº‹å‰ã«å¯¾è±¡ã‚µã‚¤ãƒˆã®è¨±å¯ã‚’å¾—ã¦ãã ã•ã„\n\n### æŠ€è¡“çš„ãªæ³¨æ„äº‹é …\n\n1. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã‚„HTTPã‚¨ãƒ©ãƒ¼ã¯è‡ªå‹•çš„ã«å‡¦ç†ã•ã‚Œã¾ã™\n2. **ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ**: é•·æ™‚é–“å¿œç­”ãŒãªã„å ´åˆã¯è‡ªå‹•çš„ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã™\n3. **æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**: UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã™\n\n## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n\n### ã‚ˆãã‚ã‚‹å•é¡Œ\n\n1. **ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„**\n   - CSSã‚»ãƒ¬ã‚¯ã‚¿ãŒæ­£ã—ã„ã‹ç¢ºèª\n   - å¯¾è±¡ã‚µã‚¤ãƒˆã®æ§‹é€ ãŒå¤‰æ›´ã•ã‚Œã¦ã„ãªã„ã‹ç¢ºèª\n   - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèª\n\n2. **ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã‚‹**\n   - User-AgentãŒé©åˆ‡ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n   - ã‚¢ã‚¯ã‚»ã‚¹é »åº¦ã‚’ä¸‹ã’ã‚‹ï¼ˆå¾…æ©Ÿæ™‚é–“ã‚’å¢—ã‚„ã™ï¼‰\n   - å¯¾è±¡ã‚µã‚¤ãƒˆãŒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ç¦æ­¢ã—ã¦ã„ãªã„ã‹ç¢ºèª\n\n3. **ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒèµ·å‹•ã—ãªã„**\n   - Pythonç’°å¢ƒã‚’ç¢ºèª\n   - ä¾å­˜é–¢ä¿‚ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n   - ãƒãƒ¼ãƒˆãŒä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ã‹ç¢ºèª\n\n## ğŸ“ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹\n\nã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯æ•™è‚²ãƒ»ç ”ç©¶ç›®çš„ã§ã®ä½¿ç”¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚\nå•†ç”¨åˆ©ç”¨ã®å ´åˆã¯ã€é©åˆ‡ãªãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n\n## ğŸ¤ è²¢çŒ®\n\nãƒã‚°å ±å‘Šã‚„æ©Ÿèƒ½æ”¹å–„ã®ææ¡ˆã¯ã€GitHubã®Issuesã¾ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚\n\n## ğŸ“ ã‚µãƒãƒ¼ãƒˆ\n\næŠ€è¡“çš„ãªè³ªå•ã‚„ã‚µãƒãƒ¼ãƒˆãŒå¿…è¦ãªå ´åˆã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã™ã‚‹ã‹ã€\nã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ã§è³ªå•ã—ã¦ãã ã•ã„ã€‚\n\n---\n\n**å…è²¬äº‹é …**: ã“ã®ãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨ã«ã‚ˆã£ã¦ç”Ÿã˜ãŸå•é¡Œã«ã¤ã„ã¦ã€é–‹ç™ºè€…ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚\nä½¿ç”¨è€…ã®è²¬ä»»ã«ãŠã„ã¦ã€é©åˆ‡ã«ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚\n'''\n\n# Save files\nwith open(f'{output_dir}/streamlit_app.py', 'w', encoding='utf-8') as f:\n    f.write(streamlit_app_code)\n\nwith open(f'{output_dir}/requirements.txt', 'w', encoding='utf-8') as f:\n    f.write(requirements_txt)\n\nwith open(f'{output_dir}/README.md', 'w', encoding='utf-8') as f:\n    f.write(readme_md)\n\nprint(\"âœ… Streamlitã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\nprint(\"\\nğŸ“ ä½œæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\")\nprint(\"- streamlit_app.py (ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³)\")\nprint(\"- requirements.txt (ä¾å­˜é–¢ä¿‚)\")\nprint(\"- README.md (ä½¿ç”¨æ–¹æ³•èª¬æ˜)\")\nprint(f\"\\nğŸ“‚ ä¿å­˜å ´æ‰€: {output_dir}\")\nprint(\"\\nğŸš€ èµ·å‹•æ–¹æ³•:\")\nprint(\"1. pip install -r requirements.txt\")\nprint(\"2. streamlit run streamlit_app.py\")\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'streamlit'",
          "traceback": [
            "---------------------------------------------------------------------------",
            "ModuleNotFoundError                       Traceback (most recent call last)",
            "Cell In[1], line 3",
            "      1 os.environ.set_envs_for_execution({***MASKED***})",
            "      2 import os",
            "----> 3 import streamlit as st",
            "      4 import requests",
            "      5 from bs4 import BeautifulSoup",
            "ModuleNotFoundError: No module named 'streamlit'",
            ""
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ""
    },
    {
      "cell_type": "code",
      "metadata": {
        "file_changes": [
          {
            "path": "/home/user/output/streamlit_app.py",
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/toolu_01MVarDi62nsDNwqggvXZLTW/output/streamlit_app.py",
              "mime_type": "text/x-python"
            },
            "change_type": "MODIFIED"
          },
          {
            "path": "/home/user/output/requirements.txt",
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/toolu_01MVarDi62nsDNwqggvXZLTW/output/requirements.txt",
              "mime_type": "text/plain"
            },
            "change_type": "MODIFIED"
          },
          {
            "path": "/home/user/output/README.md",
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/toolu_01MVarDi62nsDNwqggvXZLTW/output/README.md",
              "mime_type": "text/markdown"
            },
            "change_type": "MODIFIED"
          }
        ]
      },
      "source": [
        "import os\n\n# Create output directory if it doesn't exist\noutput_dir = '/home/user/output'\nos.makedirs(output_dir, exist_ok=True)\n\n# Create streamlit_app.py\nstreamlit_app_code = '''import streamlit as st\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\nimport random\nfrom urllib.parse import urljoin, urlparse\nimport io\nfrom datetime import datetime\n\n# Page configuration\nst.set_page_config(\n    page_title=\"å®‰å…¨ãªWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«\",\n    page_icon=\"ğŸ•·ï¸\",\n    layout=\"wide\"\n)\n\n# Custom CSS\nst.markdown(\"\"\"\n<style>\n.main-header {\n    font-size: 2.5rem;\n    color: #1f77b4;\n    text-align: center;\n    margin-bottom: 2rem;\n}\n.section-header {\n    font-size: 1.5rem;\n    color: #ff7f0e;\n    margin-top: 2rem;\n    margin-bottom: 1rem;\n}\n.info-box {\n    background-color: #f0f2f6;\n    padding: 1rem;\n    border-radius: 0.5rem;\n    border-left: 4px solid #1f77b4;\n    margin: 1rem 0;\n}\n.warning-box {\n    background-color: #fff3cd;\n    padding: 1rem;\n    border-radius: 0.5rem;\n    border-left: 4px solid #ffc107;\n    margin: 1rem 0;\n}\n.success-box {\n    background-color: #d4edda;\n    padding: 1rem;\n    border-radius: 0.5rem;\n    border-left: 4px solid #28a745;\n    margin: 1rem 0;\n}\n</style>\n\"\"\", unsafe_allow_html=True)\n\n# Title\nst.markdown('<h1 class=\"main-header\">ğŸ•·ï¸ å®‰å…¨ãªWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«</h1>', unsafe_allow_html=True)\n\n# Warning message\nst.markdown(\"\"\"\n<div class=\"warning-box\">\n<strong>âš ï¸ é‡è¦ãªæ³¨æ„äº‹é …</strong><br>\nã“ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€å¯¾è±¡ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ã¨robots.txtã‚’å¿…ãšç¢ºèªã—ã¦ãã ã•ã„ã€‚\né©åˆ‡ãªé–“éš”ã§ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’å¿ƒãŒã‘ã€ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n</div>\n\"\"\", unsafe_allow_html=True)\n\n# Sidebar for settings\nst.sidebar.markdown('<h2 class=\"section-header\">âš™ï¸ è¨­å®š</h2>', unsafe_allow_html=True)\n\n# Input fields\nbase_url = st.sidebar.text_input(\n    \"ãƒ™ãƒ¼ã‚¹URL\",\n    value=\"https://www.walkerplus.com/spot_list/ar0700/sg0107/\",\n    help=\"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®ãƒ™ãƒ¼ã‚¹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„\"\n)\n\ncss_selector = st.sidebar.text_input(\n    \"CSSã‚»ãƒ¬ã‚¯ã‚¿\",\n    value=\"a.m-mainlist-item__ttl\",\n    help=\"å–å¾—ã—ãŸã„è¦ç´ ã®CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„\"\n)\n\n# Page range\ncol1, col2 = st.sidebar.columns(2)\nwith col1:\n    start_page = st.number_input(\"é–‹å§‹ãƒšãƒ¼ã‚¸\", min_value=1, value=1)\nwith col2:\n    end_page = st.number_input(\"çµ‚äº†ãƒšãƒ¼ã‚¸\", min_value=1, value=5)\n\n# Advanced settings\nst.sidebar.markdown(\"### è©³ç´°è¨­å®š\")\nmin_delay = st.sidebar.slider(\"æœ€å°å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰\", 1, 10, 1)\nmax_delay = st.sidebar.slider(\"æœ€å¤§å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰\", 1, 10, 5)\ntimeout = st.sidebar.slider(\"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰\", 5, 30, 10)\n\n# CSS Selector Guide\nst.markdown('<h2 class=\"section-header\">ğŸ“– CSSã‚»ãƒ¬ã‚¯ã‚¿ã®å–å¾—æ–¹æ³•</h2>', unsafe_allow_html=True)\n\nwith st.expander(\"CSSã‚»ãƒ¬ã‚¯ã‚¿ã®å–å¾—æ‰‹é †\", expanded=False):\n    st.markdown(\"\"\"\n    ### æ‰‹é †ï¼š\n    1. **å¯¾è±¡ã‚µã‚¤ãƒˆã‚’é–‹ã** - ãƒ–ãƒ©ã‚¦ã‚¶ã§å¯¾è±¡ã®Webãƒšãƒ¼ã‚¸ã‚’é–‹ãã¾ã™\n    2. **è¦ç´ ã‚’å³ã‚¯ãƒªãƒƒã‚¯** - å–å¾—ã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆã‚„ãƒªãƒ³ã‚¯ã‚’å³ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™\n    3. **æ¤œè¨¼ã‚’é¸æŠ** - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ã€Œæ¤œè¨¼ã€ã¾ãŸã¯ã€Œè¦ç´ ã‚’èª¿æŸ»ã€ã‚’é¸æŠ\n    4. **è¦ç´ ã‚’ã‚³ãƒ”ãƒ¼** - é–‹ç™ºè€…ãƒ„ãƒ¼ãƒ«ã§è¦ç´ ã‚’å³ã‚¯ãƒªãƒƒã‚¯ â†’ Copy â†’ Copy selector\n    5. **ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è²¼ã‚Šä»˜ã‘** - ä¸Šè¨˜ã®å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«è²¼ã‚Šä»˜ã‘ã¾ã™\n    \n    ### ã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹ã‚»ãƒ¬ã‚¯ã‚¿ã®ä¾‹ï¼š\n    - `a` - ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯\n    - `.class-name` - ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‚’æŒã¤è¦ç´ \n    - `#id-name` - ç‰¹å®šã®IDã‚’æŒã¤è¦ç´ \n    - `h1, h2, h3` - è¦‹å‡ºã—è¦ç´ \n    - `p` - æ®µè½è¦ç´ \n    - `div.container a` - containerã‚¯ãƒ©ã‚¹å†…ã®ãƒªãƒ³ã‚¯\n    \"\"\")\n\n# Function to scrape data\ndef scrape_data(base_url, css_selector, start_page, end_page, min_delay, max_delay, timeout):\n    \"\"\"\n    Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°\n    \"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n    }\n    \n    results = []\n    progress_bar = st.progress(0)\n    status_text = st.empty()\n    \n    total_pages = end_page - start_page + 1\n    \n    for page_num in range(start_page, end_page + 1):\n        try:\n            # Construct URL\n            if '{}' in base_url:\n                url = base_url.format(page_num)\n            elif base_url.endswith('/'):\n                url = f\"{base_url}?page={page_num}\"\n            else:\n                url = f\"{base_url}&page={page_num}\"\n            \n            status_text.text(f\"ãƒšãƒ¼ã‚¸ {page_num} ã‚’å‡¦ç†ä¸­... URL: {url}\")\n            \n            # Make request\n            response = requests.get(url, headers=headers, timeout=timeout)\n            response.raise_for_status()\n            \n            # Parse HTML\n            soup = BeautifulSoup(response.content, 'html.parser')\n            elements = soup.select(css_selector)\n            \n            # Extract data\n            page_results = []\n            for i, element in enumerate(elements):\n                data = {\n                    'page': page_num,\n                    'index': i + 1,\n                    'text': element.get_text(strip=True),\n                    'href': element.get('href', ''),\n                    'full_url': urljoin(url, element.get('href', '')) if element.get('href') else '',\n                    'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n                }\n                page_results.append(data)\n            \n            results.extend(page_results)\n            \n            # Update progress\n            progress = (page_num - start_page + 1) / total_pages\n            progress_bar.progress(progress)\n            \n            status_text.text(f\"ãƒšãƒ¼ã‚¸ {page_num} å®Œäº† - {len(page_results)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\")\n            \n            # Random delay\n            if page_num < end_page:  # Don't delay after the last page\n                delay = random.uniform(min_delay, max_delay)\n                time.sleep(delay)\n                \n        except requests.exceptions.RequestException as e:\n            st.error(f\"ãƒšãƒ¼ã‚¸ {page_num} ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\")\n            continue\n        except Exception as e:\n            st.error(f\"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ (ãƒšãƒ¼ã‚¸ {page_num}): {str(e)}\")\n            continue\n    \n    progress_bar.progress(1.0)\n    status_text.text(f\"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†! åˆè¨ˆ {len(results)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚\")\n    \n    return results\n\n# Main content area\nst.markdown('<h2 class=\"section-header\">ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ</h2>', unsafe_allow_html=True)\n\n# Validation\nif st.button(\"ğŸ•·ï¸ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹\", type=\"primary\", use_container_width=True):\n    if not base_url:\n        st.error(\"ãƒ™ãƒ¼ã‚¹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\")\n    elif not css_selector:\n        st.error(\"CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\")\n    elif start_page > end_page:\n        st.error(\"é–‹å§‹ãƒšãƒ¼ã‚¸ã¯çµ‚äº†ãƒšãƒ¼ã‚¸ä»¥ä¸‹ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\")\n    else:\n        with st.spinner(\"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­...\"):\n            results = scrape_data(base_url, css_selector, start_page, end_page, min_delay, max_delay, timeout)\n        \n        if results:\n            st.success(f\"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†! {len(results)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚\")\n            \n            # Store results in session state\n            st.session_state.scraping_results = results\n            st.session_state.scraping_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n\n# Display results if available\nif 'scraping_results' in st.session_state and st.session_state.scraping_results:\n    st.markdown('<h2 class=\"section-header\">ğŸ“Š å–å¾—ãƒ‡ãƒ¼ã‚¿</h2>', unsafe_allow_html=True)\n    \n    results = st.session_state.scraping_results\n    df = pd.DataFrame(results)\n    \n    # Data preview\n    st.markdown(\"### ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼\")\n    st.dataframe(df, use_container_width=True)\n    \n    # Statistics\n    col1, col2, col3, col4 = st.columns(4)\n    with col1:\n        st.metric(\"ç·ãƒ‡ãƒ¼ã‚¿æ•°\", len(results))\n    with col2:\n        st.metric(\"ãƒšãƒ¼ã‚¸æ•°\", df['page'].nunique())\n    with col3:\n        st.metric(\"ãƒªãƒ³ã‚¯æ•°\", df[df['href'] != ''].shape[0])\n    with col4:\n        st.metric(\"ãƒ†ã‚­ã‚¹ãƒˆã®ã¿\", df[df['href'] == ''].shape[0])\n    \n    # Download section\n    st.markdown('<h2 class=\"section-header\">ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰</h2>', unsafe_allow_html=True)\n    \n    timestamp = st.session_state.scraping_timestamp\n    \n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        # Excel download\n        excel_buffer = io.BytesIO()\n        df.to_excel(excel_buffer, index=False, engine='openpyxl')\n        excel_buffer.seek(0)\n        \n        st.download_button(\n            label=\"ğŸ“Š Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\",\n            data=excel_buffer.getvalue(),\n            file_name=f\"scraping_results_{timestamp}.xlsx\",\n            mime=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n            use_container_width=True\n        )\n    \n    with col2:\n        # CSV download\n        csv_buffer = io.StringIO()\n        df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')\n        \n        st.download_button(\n            label=\"ğŸ“„ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\",\n            data=csv_buffer.getvalue(),\n            file_name=f\"scraping_results_{timestamp}.csv\",\n            mime=\"text/csv\",\n            use_container_width=True\n        )\n    \n    with col3:\n        # Text download\n        text_content = \"\\\\n\".join([f\"{item['text']} - {item['full_url']}\" for item in results if item['text']])\n        \n        st.download_button(\n            label=\"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\",\n            data=text_content,\n            file_name=f\"scraping_results_{timestamp}.txt\",\n            mime=\"text/plain\",\n            use_container_width=True\n        )\n\n# Footer\nst.markdown(\"---\")\nst.markdown(\"\"\"\n<div class=\"info-box\">\n<strong>ğŸ’¡ ä½¿ç”¨ä¸Šã®æ³¨æ„</strong><br>\nâ€¢ ã“ã®ãƒ„ãƒ¼ãƒ«ã¯æ•™è‚²ãƒ»ç ”ç©¶ç›®çš„ã§ã®ä½¿ç”¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™<br>\nâ€¢ å•†ç”¨åˆ©ç”¨ã®å ´åˆã¯ã€å¯¾è±¡ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ã‚’å¿…ãšç¢ºèªã—ã¦ãã ã•ã„<br>\nâ€¢ éåº¦ãªã‚¢ã‚¯ã‚»ã‚¹ã¯ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™<br>\nâ€¢ robots.txtãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’å°Šé‡ã—ã¦ãã ã•ã„\n</div>\n\"\"\", unsafe_allow_html=True)\n'''\n\n# Create requirements.txt\nrequirements_txt = '''streamlit>=1.28.0\nrequests>=2.31.0\nbeautifulsoup4>=4.12.0\npandas>=2.0.0\nopenpyxl>=3.1.0\nlxml>=4.9.0\n'''\n\n# Create README.md\nreadme_md = '''# å®‰å…¨ãªWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«\n\nã“ã®Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€Webã‚µã‚¤ãƒˆã‹ã‚‰å®‰å…¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n\n## ğŸš€ æ©Ÿèƒ½\n\n- **å®‰å…¨ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°**: 1-5ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿæ™‚é–“ã§ã‚µãƒ¼ãƒãƒ¼è² è·ã‚’è»½æ¸›\n- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—è¡¨ç¤º**: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®é€²è¡ŒçŠ¶æ³ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ç¢ºèª\n- **ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**: å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ãã®å ´ã§ç¢ºèª\n- **å¤šå½¢å¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**: Excelã€CSVã€ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n- **CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚¬ã‚¤ãƒ‰**: åˆå¿ƒè€…å‘ã‘ã®è©³ç´°ãªä½¿ç”¨æ–¹æ³•èª¬æ˜\n- **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: å …ç‰¢ãªã‚¨ãƒ©ãƒ¼å‡¦ç†æ©Ÿèƒ½\n\n## ğŸ“‹ è¦ä»¶\n\n- Python 3.8ä»¥ä¸Š\n- å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯`requirements.txt`ã«è¨˜è¼‰\n\n## ğŸ› ï¸ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n\n1. ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã¾ãŸã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n```bash\ngit clone <repository-url>\ncd web-scraping-tool\n```\n\n2. ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n```bash\npip install -r requirements.txt\n```\n\n3. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•\n```bash\nstreamlit run streamlit_app.py\n```\n\n## ğŸ“– ä½¿ç”¨æ–¹æ³•\n\n### åŸºæœ¬çš„ãªä½¿ã„æ–¹\n\n1. **ãƒ™ãƒ¼ã‚¹URLå…¥åŠ›**: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®Webã‚µã‚¤ãƒˆã®URLã‚’å…¥åŠ›\n2. **CSSã‚»ãƒ¬ã‚¯ã‚¿å…¥åŠ›**: å–å¾—ã—ãŸã„è¦ç´ ã®CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’å…¥åŠ›\n3. **ãƒšãƒ¼ã‚¸ç¯„å›²è¨­å®š**: é–‹å§‹ãƒšãƒ¼ã‚¸ã¨çµ‚äº†ãƒšãƒ¼ã‚¸ã‚’æŒ‡å®š\n4. **ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ**: ã€Œã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯\n\n### CSSã‚»ãƒ¬ã‚¯ã‚¿ã®å–å¾—æ–¹æ³•\n\n1. å¯¾è±¡ã‚µã‚¤ãƒˆã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã\n2. å–å¾—ã—ãŸã„è¦ç´ ã‚’å³ã‚¯ãƒªãƒƒã‚¯\n3. ã€Œæ¤œè¨¼ã€ã¾ãŸã¯ã€Œè¦ç´ ã‚’èª¿æŸ»ã€ã‚’é¸æŠ\n4. é–‹ç™ºè€…ãƒ„ãƒ¼ãƒ«ã§è¦ç´ ã‚’å³ã‚¯ãƒªãƒƒã‚¯ â†’ Copy â†’ Copy selector\n5. å–å¾—ã—ãŸã‚»ãƒ¬ã‚¯ã‚¿ã‚’å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«è²¼ã‚Šä»˜ã‘\n\n### ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š\n\n- **ãƒ™ãƒ¼ã‚¹URL**: `https://www.walkerplus.com/spot_list/ar0700/sg0107/`\n- **CSSã‚»ãƒ¬ã‚¯ã‚¿**: `a.m-mainlist-item__ttl`\n- **ãƒšãƒ¼ã‚¸ç¯„å›²**: 1-5ãƒšãƒ¼ã‚¸\n- **å¾…æ©Ÿæ™‚é–“**: 1-5ç§’ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰\n\n## âš™ï¸ è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³\n\n### åŸºæœ¬è¨­å®š\n- **ãƒ™ãƒ¼ã‚¹URL**: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®URL\n- **CSSã‚»ãƒ¬ã‚¯ã‚¿**: å–å¾—ã™ã‚‹è¦ç´ ã®æŒ‡å®š\n- **ãƒšãƒ¼ã‚¸ç¯„å›²**: é–‹å§‹ãƒ»çµ‚äº†ãƒšãƒ¼ã‚¸ã®æŒ‡å®š\n\n### è©³ç´°è¨­å®š\n- **æœ€å°å¾…æ©Ÿæ™‚é–“**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®æœ€å°å¾…æ©Ÿæ™‚é–“ï¼ˆ1-10ç§’ï¼‰\n- **æœ€å¤§å¾…æ©Ÿæ™‚é–“**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®æœ€å¤§å¾…æ©Ÿæ™‚é–“ï¼ˆ1-10ç§’ï¼‰\n- **ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆ5-30ç§’ï¼‰\n\n## ğŸ“Š å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿\n\nå–å¾—ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã«ã¯ä»¥ä¸‹ã®æƒ…å ±ãŒå«ã¾ã‚Œã¾ã™ï¼š\n\n- **page**: ãƒšãƒ¼ã‚¸ç•ªå·\n- **index**: ãƒšãƒ¼ã‚¸å†…ã§ã®è¦ç´ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n- **text**: è¦ç´ ã®ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹\n- **href**: ãƒªãƒ³ã‚¯ã®hrefå±æ€§ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n- **full_url**: å®Œå…¨ãªURLï¼ˆç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›ï¼‰\n- **scraped_at**: ãƒ‡ãƒ¼ã‚¿å–å¾—æ—¥æ™‚\n\n## ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å½¢å¼\n\n- **Excel (.xlsx)**: è¡¨å½¢å¼ã§ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿\n- **CSV (.csv)**: æ±ç”¨çš„ãªãƒ‡ãƒ¼ã‚¿å½¢å¼\n- **ãƒ†ã‚­ã‚¹ãƒˆ (.txt)**: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆå½¢å¼\n\n## âš ï¸ é‡è¦ãªæ³¨æ„äº‹é …\n\n### æ³•çš„ãƒ»å€«ç†çš„ãªè€ƒæ…®äº‹é …\n\n1. **åˆ©ç”¨è¦ç´„ã®ç¢ºèª**: å¯¾è±¡ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ã‚’å¿…ãšç¢ºèªã—ã¦ãã ã•ã„\n2. **robots.txtã®å°Šé‡**: ã‚µã‚¤ãƒˆã®robots.txtãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¢ºèªã—ã€å°Šé‡ã—ã¦ãã ã•ã„\n3. **é©åˆ‡ãªé–“éš”**: ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†ã€é©åˆ‡ãªé–“éš”ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãã ã•ã„\n4. **å•†ç”¨åˆ©ç”¨**: å•†ç”¨ç›®çš„ã§ã®ä½¿ç”¨ã¯ã€äº‹å‰ã«å¯¾è±¡ã‚µã‚¤ãƒˆã®è¨±å¯ã‚’å¾—ã¦ãã ã•ã„\n\n### æŠ€è¡“çš„ãªæ³¨æ„äº‹é …\n\n1. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã‚„HTTPã‚¨ãƒ©ãƒ¼ã¯è‡ªå‹•çš„ã«å‡¦ç†ã•ã‚Œã¾ã™\n2. **ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ**: é•·æ™‚é–“å¿œç­”ãŒãªã„å ´åˆã¯è‡ªå‹•çš„ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã™\n3. **æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**: UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã™\n\n## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n\n### ã‚ˆãã‚ã‚‹å•é¡Œ\n\n1. **ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„**\n   - CSSã‚»ãƒ¬ã‚¯ã‚¿ãŒæ­£ã—ã„ã‹ç¢ºèª\n   - å¯¾è±¡ã‚µã‚¤ãƒˆã®æ§‹é€ ãŒå¤‰æ›´ã•ã‚Œã¦ã„ãªã„ã‹ç¢ºèª\n   - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèª\n\n2. **ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã‚‹**\n   - User-AgentãŒé©åˆ‡ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n   - ã‚¢ã‚¯ã‚»ã‚¹é »åº¦ã‚’ä¸‹ã’ã‚‹ï¼ˆå¾…æ©Ÿæ™‚é–“ã‚’å¢—ã‚„ã™ï¼‰\n   - å¯¾è±¡ã‚µã‚¤ãƒˆãŒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ç¦æ­¢ã—ã¦ã„ãªã„ã‹ç¢ºèª\n\n3. **ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒèµ·å‹•ã—ãªã„**\n   - Pythonç’°å¢ƒã‚’ç¢ºèª\n   - ä¾å­˜é–¢ä¿‚ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n   - ãƒãƒ¼ãƒˆãŒä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ã‹ç¢ºèª\n\n## ğŸ“ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹\n\nã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯æ•™è‚²ãƒ»ç ”ç©¶ç›®çš„ã§ã®ä½¿ç”¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚\nå•†ç”¨åˆ©ç”¨ã®å ´åˆã¯ã€é©åˆ‡ãªãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n\n## ğŸ¤ è²¢çŒ®\n\nãƒã‚°å ±å‘Šã‚„æ©Ÿèƒ½æ”¹å–„ã®ææ¡ˆã¯ã€GitHubã®Issuesã¾ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚\n\n## ğŸ“ ã‚µãƒãƒ¼ãƒˆ\n\næŠ€è¡“çš„ãªè³ªå•ã‚„ã‚µãƒãƒ¼ãƒˆãŒå¿…è¦ãªå ´åˆã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã™ã‚‹ã‹ã€\nã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ã§è³ªå•ã—ã¦ãã ã•ã„ã€‚\n\n---\n\n**å…è²¬äº‹é …**: ã“ã®ãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨ã«ã‚ˆã£ã¦ç”Ÿã˜ãŸå•é¡Œã«ã¤ã„ã¦ã€é–‹ç™ºè€…ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚\nä½¿ç”¨è€…ã®è²¬ä»»ã«ãŠã„ã¦ã€é©åˆ‡ã«ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚\n'''\n\n# Save files\nwith open(f'{output_dir}/streamlit_app.py', 'w', encoding='utf-8') as f:\n    f.write(streamlit_app_code)\n\nwith open(f'{output_dir}/requirements.txt', 'w', encoding='utf-8') as f:\n    f.write(requirements_txt)\n\nwith open(f'{output_dir}/README.md', 'w', encoding='utf-8') as f:\n    f.write(readme_md)\n\nprint(\"âœ… Streamlitã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\nprint(\"\\nğŸ“ ä½œæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\")\nprint(\"- streamlit_app.py (ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³)\")\nprint(\"- requirements.txt (ä¾å­˜é–¢ä¿‚)\")\nprint(\"- README.md (ä½¿ç”¨æ–¹æ³•èª¬æ˜)\")\nprint(f\"\\nğŸ“‚ ä¿å­˜å ´æ‰€: {output_dir}\")\nprint(\"\\nğŸš€ èµ·å‹•æ–¹æ³•:\")\nprint(\"1. pip install -r requirements.txt\")\nprint(\"2. streamlit run streamlit_app.py\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Streamlitã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼\n\nğŸ“ ä½œæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\n- streamlit_app.py (ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³)\n- requirements.txt (ä¾å­˜é–¢ä¿‚)\n- README.md (ä½¿ç”¨æ–¹æ³•èª¬æ˜)\n\nğŸ“‚ ä¿å­˜å ´æ‰€: /home/user/output\n\nğŸš€ èµ·å‹•æ–¹æ³•:\n1. pip install -r requirements.txt\n2. streamlit run streamlit_app.py\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ""
    },
    {
      "cell_type": "code",
      "metadata": {
        "file_changes": [
          {
            "path": "/home/user/output/.streamlit/config.toml",
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/toolu_0149x1m5YnbfBeuMwepGpr46/output/.streamlit/config.toml",
              "mime_type": "application/octet-stream"
            },
            "change_type": "MODIFIED"
          },
          {
            "path": "/home/user/output/.gitignore",
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/toolu_0149x1m5YnbfBeuMwepGpr46/output/.gitignore",
              "mime_type": "application/octet-stream"
            },
            "change_type": "MODIFIED"
          },
          {
            "path": "/home/user/output/setup.sh",
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/toolu_0149x1m5YnbfBeuMwepGpr46/output/setup.sh",
              "mime_type": "text/x-sh"
            },
            "change_type": "MODIFIED"
          },
          {
            "path": "/home/user/output/README.md",
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/toolu_0149x1m5YnbfBeuMwepGpr46/output/README.md",
              "mime_type": "text/markdown"
            },
            "change_type": "MODIFIED"
          }
        ]
      },
      "source": [
        "import os\n\n# Create output directory if it doesn't exist\noutput_dir = '/home/user/output'\nos.makedirs(output_dir, exist_ok=True)\n\n# Create .streamlit directory\nstreamlit_config_dir = os.path.join(output_dir, '.streamlit')\nos.makedirs(streamlit_config_dir, exist_ok=True)\n\n# Create .streamlit/config.toml\nconfig_toml = '''[global]\n# Streamlit configuration for production deployment\n\n# Development mode\ndevelopmentMode = false\n\n# Show warning about running in development mode\nshowWarningOnDirectExecution = false\n\n[server]\n# Server configuration\nheadless = true\nenableCORS = false\nenableXsrfProtection = true\n\n# Port configuration (Streamlit Cloud will override this)\nport = 8501\n\n# File watcher configuration\nfileWatcherType = \"none\"\n\n# Max upload size (200MB)\nmaxUploadSize = 200\n\n# Max message size (200MB)\nmaxMessageSize = 200\n\n[browser]\n# Browser configuration\ngatherUsageStats = false\nserverAddress = \"0.0.0.0\"\n\n[theme]\n# Custom theme configuration\nprimaryColor = \"#1f77b4\"\nbackgroundColor = \"#ffffff\"\nsecondaryBackgroundColor = \"#f0f2f6\"\ntextColor = \"#262730\"\nfont = \"sans serif\"\n\n[client]\n# Client configuration\ncaching = true\ndisplayEnabled = true\nshowErrorDetails = true\n\n[runner]\n# Runner configuration\nmagicEnabled = true\ninstallTracer = false\nfixMatplotlib = true\n\n[logger]\n# Logging configuration\nlevel = \"info\"\nmessageFormat = \"%(asctime)s %(message)s\"\n\n[deprecation]\n# Deprecation warnings\nshowPyplotGlobalUse = false\n'''\n\n# Create .gitignore\ngitignore_content = '''# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# Streamlit specific\n.streamlit/secrets.toml\n\n# IDE specific files\n.vscode/\n.idea/\n*.swp\n*.swo\n*~\n\n# OS specific files\n.DS_Store\n.DS_Store?\n._*\n.Spotlight-V100\n.Trashes\nehthumbs.db\nThumbs.db\n\n# Temporary files\n*.tmp\n*.temp\ntemp/\ntmp/\n\n# Log files\n*.log\n\n# Data files (optional - uncomment if you don't want to track data files)\n# *.csv\n# *.xlsx\n# *.json\n# *.xml\n\n# Output files\noutput/\nresults/\ndownloads/\n\n# Cache directories\n.cache/\n__pycache__/\n\n# Virtual environment\nvenv/\nenv/\n.venv/\n.env/\n\n# Local configuration\nconfig.local.toml\nsecrets.local.toml\n'''\n\n# Create setup.sh (optional setup script)\nsetup_sh = '''#!/bin/bash\n\n# Setup script for Streamlit Web Scraping Tool\n# This script is optional and can be used for additional setup if needed\n\necho \"ğŸš€ Setting up Streamlit Web Scraping Tool...\"\n\n# Update system packages (if running on Linux)\nif command -v apt-get &> /dev/null; then\n    echo \"ğŸ“¦ Updating system packages...\"\n    sudo apt-get update\nfi\n\n# Install Python dependencies\necho \"ğŸ“‹ Installing Python dependencies...\"\npip install -r requirements.txt\n\n# Create necessary directories\necho \"ğŸ“ Creating directories...\"\nmkdir -p logs\nmkdir -p data\nmkdir -p output\n\n# Set permissions\necho \"ğŸ” Setting permissions...\"\nchmod +x setup.sh\n\n# Verify installation\necho \"âœ… Verifying installation...\"\npython -c \"import streamlit; print(f'Streamlit version: {streamlit.__version__}')\"\npython -c \"import requests; print(f'Requests version: {requests.__version__}')\"\npython -c \"import pandas; print(f'Pandas version: {pandas.__version__}')\"\npython -c \"import bs4; print(f'BeautifulSoup version: {bs4.__version__}')\"\n\necho \"ğŸ‰ Setup completed successfully!\"\necho \"\"\necho \"To run the application:\"\necho \"  streamlit run streamlit_app.py\"\necho \"\"\necho \"To deploy to Streamlit Cloud:\"\necho \"  1. Push your code to GitHub\"\necho \"  2. Go to https://share.streamlit.io/\"\necho \"  3. Connect your GitHub repository\"\necho \"  4. Deploy!\"\n'''\n\n# Update README.md with deployment instructions\nupdated_readme = '''# å®‰å…¨ãªWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«\n\nã“ã®Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€Webã‚µã‚¤ãƒˆã‹ã‚‰å®‰å…¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n\n## ğŸš€ æ©Ÿèƒ½\n\n- **å®‰å…¨ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°**: 1-5ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿæ™‚é–“ã§ã‚µãƒ¼ãƒãƒ¼è² è·ã‚’è»½æ¸›\n- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—è¡¨ç¤º**: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®é€²è¡ŒçŠ¶æ³ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ç¢ºèª\n- **ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**: å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ãã®å ´ã§ç¢ºèª\n- **å¤šå½¢å¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**: Excelã€CSVã€ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n- **CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚¬ã‚¤ãƒ‰**: åˆå¿ƒè€…å‘ã‘ã®è©³ç´°ãªä½¿ç”¨æ–¹æ³•èª¬æ˜\n- **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: å …ç‰¢ãªã‚¨ãƒ©ãƒ¼å‡¦ç†æ©Ÿèƒ½\n\n## ğŸ“‹ è¦ä»¶\n\n- Python 3.8ä»¥ä¸Š\n- å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯`requirements.txt`ã«è¨˜è¼‰\n\n## ğŸ› ï¸ ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n\n### æ–¹æ³•1: æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n\n1. ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã¾ãŸã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n```bash\ngit clone <repository-url>\ncd web-scraping-tool\n```\n\n2. ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n```bash\npip install -r requirements.txt\n```\n\n3. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•\n```bash\nstreamlit run streamlit_app.py\n```\n\n### æ–¹æ³•2: ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨\n\n1. ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³\n```bash\ngit clone <repository-url>\ncd web-scraping-tool\n```\n\n2. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n\n3. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•\n```bash\nstreamlit run streamlit_app.py\n```\n\n## â˜ï¸ Streamlit Cloudã§ã®ãƒ‡ãƒ—ãƒ­ã‚¤\n\n### å‰ææ¡ä»¶\n- GitHubã‚¢ã‚«ã‚¦ãƒ³ãƒˆ\n- Streamlit Cloudã‚¢ã‚«ã‚¦ãƒ³ãƒˆï¼ˆç„¡æ–™ï¼‰\n\n### ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †\n\n1. **GitHubãƒªãƒã‚¸ãƒˆãƒªã®æº–å‚™**\n   ```bash\n   # æ–°ã—ã„ãƒªãƒã‚¸ãƒˆãƒªã‚’ä½œæˆ\n   git init\n   git add .\n   git commit -m \"Initial commit: Streamlit web scraping tool\"\n   \n   # GitHubã«ãƒ—ãƒƒã‚·ãƒ¥\n   git remote add origin https://github.com/yourusername/web-scraping-tool.git\n   git branch -M main\n   git push -u origin main\n   ```\n\n2. **Streamlit Cloudã§ã®ãƒ‡ãƒ—ãƒ­ã‚¤**\n   - [Streamlit Cloud](https://share.streamlit.io/) ã«ã‚¢ã‚¯ã‚»ã‚¹\n   - GitHubã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ã‚µã‚¤ãƒ³ã‚¤ãƒ³\n   - \"New app\" ã‚’ã‚¯ãƒªãƒƒã‚¯\n   - ãƒªãƒã‚¸ãƒˆãƒªã‚’é¸æŠ: `yourusername/web-scraping-tool`\n   - ãƒ–ãƒ©ãƒ³ãƒ: `main`\n   - ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«: `streamlit_app.py`\n   - \"Deploy!\" ã‚’ã‚¯ãƒªãƒƒã‚¯\n\n3. **ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†**\n   - æ•°åˆ†ã§ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚Œã¾ã™\n   - è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸURLã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã«ãªã‚Šã¾ã™\n   - ä¾‹: `https://yourusername-web-scraping-tool-streamlit-app-xyz123.streamlit.app/`\n\n### ãƒ‡ãƒ—ãƒ­ã‚¤æ™‚ã®æ³¨æ„äº‹é …\n\n- **requirements.txt**: å¿…è¦ãªä¾å­˜é–¢ä¿‚ãŒã™ã¹ã¦è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª\n- **.streamlit/config.toml**: Streamlit Cloudã«æœ€é©åŒ–ã•ã‚ŒãŸè¨­å®š\n- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹å ´åˆã¯æ³¨æ„\n- **å®Ÿè¡Œæ™‚é–“**: é•·æ™‚é–“å®Ÿè¡Œã•ã‚Œã‚‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯é¿ã‘ã‚‹\n\n## ğŸ“– ä½¿ç”¨æ–¹æ³•\n\n### åŸºæœ¬çš„ãªä½¿ã„æ–¹\n\n1. **ãƒ™ãƒ¼ã‚¹URLå…¥åŠ›**: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®Webã‚µã‚¤ãƒˆã®URLã‚’å…¥åŠ›\n2. **CSSã‚»ãƒ¬ã‚¯ã‚¿å…¥åŠ›**: å–å¾—ã—ãŸã„è¦ç´ ã®CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’å…¥åŠ›\n3. **ãƒšãƒ¼ã‚¸ç¯„å›²è¨­å®š**: é–‹å§‹ãƒšãƒ¼ã‚¸ã¨çµ‚äº†ãƒšãƒ¼ã‚¸ã‚’æŒ‡å®š\n4. **ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ**: ã€Œã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯\n\n### CSSã‚»ãƒ¬ã‚¯ã‚¿ã®å–å¾—æ–¹æ³•\n\n1. å¯¾è±¡ã‚µã‚¤ãƒˆã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã\n2. å–å¾—ã—ãŸã„è¦ç´ ã‚’å³ã‚¯ãƒªãƒƒã‚¯\n3. ã€Œæ¤œè¨¼ã€ã¾ãŸã¯ã€Œè¦ç´ ã‚’èª¿æŸ»ã€ã‚’é¸æŠ\n4. é–‹ç™ºè€…ãƒ„ãƒ¼ãƒ«ã§è¦ç´ ã‚’å³ã‚¯ãƒªãƒƒã‚¯ â†’ Copy â†’ Copy selector\n5. å–å¾—ã—ãŸã‚»ãƒ¬ã‚¯ã‚¿ã‚’å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«è²¼ã‚Šä»˜ã‘\n\n### ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š\n\n- **ãƒ™ãƒ¼ã‚¹URL**: `https://www.walkerplus.com/spot_list/ar0700/sg0107/`\n- **CSSã‚»ãƒ¬ã‚¯ã‚¿**: `a.m-mainlist-item__ttl`\n- **ãƒšãƒ¼ã‚¸ç¯„å›²**: 1-5ãƒšãƒ¼ã‚¸\n- **å¾…æ©Ÿæ™‚é–“**: 1-5ç§’ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰\n\n## âš™ï¸ è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³\n\n### åŸºæœ¬è¨­å®š\n- **ãƒ™ãƒ¼ã‚¹URL**: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®URL\n- **CSSã‚»ãƒ¬ã‚¯ã‚¿**: å–å¾—ã™ã‚‹è¦ç´ ã®æŒ‡å®š\n- **ãƒšãƒ¼ã‚¸ç¯„å›²**: é–‹å§‹ãƒ»çµ‚äº†ãƒšãƒ¼ã‚¸ã®æŒ‡å®š\n\n### è©³ç´°è¨­å®š\n- **æœ€å°å¾…æ©Ÿæ™‚é–“**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®æœ€å°å¾…æ©Ÿæ™‚é–“ï¼ˆ1-10ç§’ï¼‰\n- **æœ€å¤§å¾…æ©Ÿæ™‚é–“**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®æœ€å¤§å¾…æ©Ÿæ™‚é–“ï¼ˆ1-10ç§’ï¼‰\n- **ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆ5-30ç§’ï¼‰\n\n## ğŸ“Š å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿\n\nå–å¾—ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã«ã¯ä»¥ä¸‹ã®æƒ…å ±ãŒå«ã¾ã‚Œã¾ã™ï¼š\n\n- **page**: ãƒšãƒ¼ã‚¸ç•ªå·\n- **index**: ãƒšãƒ¼ã‚¸å†…ã§ã®è¦ç´ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n- **text**: è¦ç´ ã®ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹\n- **href**: ãƒªãƒ³ã‚¯ã®hrefå±æ€§ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n- **full_url**: å®Œå…¨ãªURLï¼ˆç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›ï¼‰\n- **scraped_at**: ãƒ‡ãƒ¼ã‚¿å–å¾—æ—¥æ™‚\n\n## ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å½¢å¼\n\n- **Excel (.xlsx)**: è¡¨å½¢å¼ã§ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿\n- **CSV (.csv)**: æ±ç”¨çš„ãªãƒ‡ãƒ¼ã‚¿å½¢å¼\n- **ãƒ†ã‚­ã‚¹ãƒˆ (.txt)**: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆå½¢å¼\n\n## ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ\n\n```\nweb-scraping-tool/\nâ”œâ”€â”€ streamlit_app.py          # ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³\nâ”œâ”€â”€ requirements.txt          # Pythonä¾å­˜é–¢ä¿‚\nâ”œâ”€â”€ README.md                # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«\nâ”œâ”€â”€ setup.sh                 # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\nâ”œâ”€â”€ .gitignore              # Gité™¤å¤–ãƒ•ã‚¡ã‚¤ãƒ«\nâ””â”€â”€ .streamlit/\n    â””â”€â”€ config.toml         # Streamlitè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«\n```\n\n## âš ï¸ é‡è¦ãªæ³¨æ„äº‹é …\n\n### æ³•çš„ãƒ»å€«ç†çš„ãªè€ƒæ…®äº‹é …\n\n1. **åˆ©ç”¨è¦ç´„ã®ç¢ºèª**: å¯¾è±¡ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ã‚’å¿…ãšç¢ºèªã—ã¦ãã ã•ã„\n2. **robots.txtã®å°Šé‡**: ã‚µã‚¤ãƒˆã®robots.txtãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¢ºèªã—ã€å°Šé‡ã—ã¦ãã ã•ã„\n3. **é©åˆ‡ãªé–“éš”**: ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†ã€é©åˆ‡ãªé–“éš”ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãã ã•ã„\n4. **å•†ç”¨åˆ©ç”¨**: å•†ç”¨ç›®çš„ã§ã®ä½¿ç”¨ã¯ã€äº‹å‰ã«å¯¾è±¡ã‚µã‚¤ãƒˆã®è¨±å¯ã‚’å¾—ã¦ãã ã•ã„\n\n### æŠ€è¡“çš„ãªæ³¨æ„äº‹é …\n\n1. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã‚„HTTPã‚¨ãƒ©ãƒ¼ã¯è‡ªå‹•çš„ã«å‡¦ç†ã•ã‚Œã¾ã™\n2. **ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ**: é•·æ™‚é–“å¿œç­”ãŒãªã„å ´åˆã¯è‡ªå‹•çš„ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã™\n3. **æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**: UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã™\n4. **Streamlit Cloudåˆ¶é™**: ç„¡æ–™ãƒ—ãƒ©ãƒ³ã§ã¯å®Ÿè¡Œæ™‚é–“ã¨ãƒ¡ãƒ¢ãƒªã«åˆ¶é™ãŒã‚ã‚Šã¾ã™\n\n## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n\n### ã‚ˆãã‚ã‚‹å•é¡Œ\n\n1. **ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„**\n   - CSSã‚»ãƒ¬ã‚¯ã‚¿ãŒæ­£ã—ã„ã‹ç¢ºèª\n   - å¯¾è±¡ã‚µã‚¤ãƒˆã®æ§‹é€ ãŒå¤‰æ›´ã•ã‚Œã¦ã„ãªã„ã‹ç¢ºèª\n   - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèª\n\n2. **ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã‚‹**\n   - User-AgentãŒé©åˆ‡ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n   - ã‚¢ã‚¯ã‚»ã‚¹é »åº¦ã‚’ä¸‹ã’ã‚‹ï¼ˆå¾…æ©Ÿæ™‚é–“ã‚’å¢—ã‚„ã™ï¼‰\n   - å¯¾è±¡ã‚µã‚¤ãƒˆãŒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ç¦æ­¢ã—ã¦ã„ãªã„ã‹ç¢ºèª\n\n3. **ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒèµ·å‹•ã—ãªã„**\n   - Pythonç’°å¢ƒã‚’ç¢ºèª\n   - ä¾å­˜é–¢ä¿‚ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n   - ãƒãƒ¼ãƒˆãŒä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ã‹ç¢ºèª\n\n4. **Streamlit Cloudã§ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¨ãƒ©ãƒ¼**\n   - requirements.txtã®å†…å®¹ã‚’ç¢ºèª\n   - Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®äº’æ›æ€§ã‚’ç¢ºèª\n   - ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’æŠŠæ¡\n\n### Streamlit Cloudå›ºæœ‰ã®å•é¡Œ\n\n1. **ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼**\n   - å‡¦ç†ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°ã‚’æ¸›ã‚‰ã™\n   - ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†æ–¹æ³•ã‚’æœ€é©åŒ–ã™ã‚‹\n\n2. **ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼**\n   - ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®å®Ÿè¡Œæ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹\n   - å¾…æ©Ÿæ™‚é–“ã‚’èª¿æ•´ã™ã‚‹\n\n3. **ä¾å­˜é–¢ä¿‚ã‚¨ãƒ©ãƒ¼**\n   - requirements.txtã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèª\n   - ä¸è¦ãªä¾å­˜é–¢ä¿‚ã‚’å‰Šé™¤\n\n## ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–\n\n### ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ\n- å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹å ´åˆã¯ã€ãƒãƒƒãƒå‡¦ç†ã‚’æ¤œè¨\n- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–ã—ã€å¿…è¦ã«å¿œã˜ã¦æœ€é©åŒ–\n\n### Streamlit Cloud\n- ç„¡æ–™ãƒ—ãƒ©ãƒ³ã®åˆ¶é™å†…ã§å‹•ä½œã™ã‚‹ã‚ˆã†è¨­è¨ˆ\n- é•·æ™‚é–“å®Ÿè¡Œã‚’é¿ã‘ã€é©åˆ‡ãªãƒšãƒ¼ã‚¸æ•°åˆ¶é™ã‚’è¨­å®š\n\n## ğŸ“ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹\n\nã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯æ•™è‚²ãƒ»ç ”ç©¶ç›®çš„ã§ã®ä½¿ç”¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚\nå•†ç”¨åˆ©ç”¨ã®å ´åˆã¯ã€é©åˆ‡ãªãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n\n## ğŸ¤ è²¢çŒ®\n\nãƒã‚°å ±å‘Šã‚„æ©Ÿèƒ½æ”¹å–„ã®ææ¡ˆã¯ã€GitHubã®Issuesã¾ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚\n\n### é–‹ç™ºã«å‚åŠ ã™ã‚‹å ´åˆ\n\n1. ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ•ã‚©ãƒ¼ã‚¯\n2. æ©Ÿèƒ½ãƒ–ãƒ©ãƒ³ãƒã‚’ä½œæˆ (`git checkout -b feature/amazing-feature`)\n3. å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆ (`git commit -m 'Add some amazing feature'`)\n4. ãƒ–ãƒ©ãƒ³ãƒã«ãƒ—ãƒƒã‚·ãƒ¥ (`git push origin feature/amazing-feature`)\n5. ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ\n\n## ğŸ“ ã‚µãƒãƒ¼ãƒˆ\n\næŠ€è¡“çš„ãªè³ªå•ã‚„ã‚µãƒãƒ¼ãƒˆãŒå¿…è¦ãªå ´åˆã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã™ã‚‹ã‹ã€\nã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ã§è³ªå•ã—ã¦ãã ã•ã„ã€‚\n\n## ğŸ”— é–¢é€£ãƒªãƒ³ã‚¯\n\n- [Streamlit Documentation](https://docs.streamlit.io/)\n- [Streamlit Cloud](https://share.streamlit.io/)\n- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n- [Requests Documentation](https://docs.python-requests.org/)\n- [Pandas Documentation](https://pandas.pydata.org/docs/)\n\n---\n\n**å…è²¬äº‹é …**: ã“ã®ãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨ã«ã‚ˆã£ã¦ç”Ÿã˜ãŸå•é¡Œã«ã¤ã„ã¦ã€é–‹ç™ºè€…ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚\nä½¿ç”¨è€…ã®è²¬ä»»ã«ãŠã„ã¦ã€é©åˆ‡ã«ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚\n'''\n\n# Save all files\nwith open(os.path.join(streamlit_config_dir, 'config.toml'), 'w', encoding='utf-8') as f:\n    f.write(config_toml)\n\nwith open(f'{output_dir}/.gitignore', 'w', encoding='utf-8') as f:\n    f.write(gitignore_content)\n\nwith open(f'{output_dir}/setup.sh', 'w', encoding='utf-8') as f:\n    f.write(setup_sh)\n\n# Update README.md\nwith open(f'{output_dir}/README.md', 'w', encoding='utf-8') as f:\n    f.write(updated_readme)\n\n# Make setup.sh executable (on Unix-like systems)\ntry:\n    import stat\n    setup_path = f'{output_dir}/setup.sh'\n    current_permissions = os.stat(setup_path).st_mode\n    os.chmod(setup_path, current_permissions | stat.S_IEXEC)\nexcept:\n    pass  # Windows or other systems where chmod might not work\n\nprint(\"âœ… Streamlit Cloudç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\nprint(\"\\nğŸ“ æ–°ã—ãä½œæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\")\nprint(\"- .streamlit/config.toml (Streamlitè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«)\")\nprint(\"- .gitignore (Gité™¤å¤–ãƒ•ã‚¡ã‚¤ãƒ«)\")\nprint(\"- setup.sh (ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ)\")\nprint(\"- README.md (æ›´æ–°ç‰ˆ - ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †è¿½åŠ )\")\n\nprint(f\"\\nğŸ“‚ ä¿å­˜å ´æ‰€: {output_dir}\")\n\nprint(\"\\nğŸš€ ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †:\")\nprint(\"1. GitHubãƒªãƒã‚¸ãƒˆãƒªã‚’ä½œæˆ\")\nprint(\"2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ—ãƒƒã‚·ãƒ¥:\")\nprint(\"   git init\")\nprint(\"   git add .\")\nprint(\"   git commit -m 'Initial commit'\")\nprint(\"   git remote add origin <your-repo-url>\")\nprint(\"   git push -u origin main\")\nprint(\"3. https://share.streamlit.io/ ã§ãƒ‡ãƒ—ãƒ­ã‚¤\")\n\nprint(\"\\nğŸ“‹ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ:\")\nprint(\"web-scraping-tool/\")\nprint(\"â”œâ”€â”€ streamlit_app.py\")\nprint(\"â”œâ”€â”€ requirements.txt\")\nprint(\"â”œâ”€â”€ README.md\")\nprint(\"â”œâ”€â”€ setup.sh\")\nprint(\"â”œâ”€â”€ .gitignore\")\nprint(\"â””â”€â”€ .streamlit/\")\nprint(\"    â””â”€â”€ config.toml\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Streamlit Cloudç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼\n\nğŸ“ æ–°ã—ãä½œæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\n- .streamlit/config.toml (Streamlitè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«)\n- .gitignore (Gité™¤å¤–ãƒ•ã‚¡ã‚¤ãƒ«)\n- setup.sh (ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ)\n- README.md (æ›´æ–°ç‰ˆ - ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †è¿½åŠ )\n\nğŸ“‚ ä¿å­˜å ´æ‰€: /home/user/output\n\nğŸš€ ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †:\n1. GitHubãƒªãƒã‚¸ãƒˆãƒªã‚’ä½œæˆ\n2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ—ãƒƒã‚·ãƒ¥:\n   git init\n   git add .\n   git commit -m 'Initial commit'\n   git remote add origin <your-repo-url>\n   git push -u origin main\n3. https://share.streamlit.io/ ã§ãƒ‡ãƒ—ãƒ­ã‚¤\n\nğŸ“‹ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ:\nweb-scraping-tool/\nâ”œâ”€â”€ streamlit_app.py\nâ”œâ”€â”€ requirements.txt\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ setup.sh\nâ”œâ”€â”€ .gitignore\nâ””â”€â”€ .streamlit/\n    â””â”€â”€ config.toml\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ""
    },
    {
      "cell_type": "code",
      "metadata": {
        "file_changes": [
          {
            "path": "/home/user/output/project_summary.json",
            "cdn_resource": {
              "url": "https://cdn1.genspark.ai/user-upload-image/jupyter/toolu_01Lz6fW5WDUuK7ot6DMna7Ur/output/project_summary.json",
              "mime_type": "application/json"
            },
            "change_type": "MODIFIED"
          }
        ]
      },
      "source": [
        "import os\nimport json\n\n# Check the output directory and list all files\noutput_dir = '/home/user/output'\n\nprint(\"ğŸ” ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯\")\nprint(\"=\" * 50)\n\n# Get all files in the output directory\ndef get_all_files(directory):\n    files_info = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            relative_path = os.path.relpath(file_path, directory)\n            file_size = os.path.getsize(file_path)\n            files_info.append({\n                'path': relative_path,\n                'size': file_size,\n                'full_path': file_path\n            })\n    return files_info\n\nfiles_info = get_all_files(output_dir)\n\nprint(f\"\\nğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ ({output_dir}):\")\nprint(\"-\" * 30)\n\n# Display file structure\nfor file_info in sorted(files_info, key=lambda x: x['path']):\n    size_kb = file_info['size'] / 1024\n    print(f\"â”œâ”€â”€ {file_info['path']} ({size_kb:.1f} KB)\")\n\nprint(f\"\\nğŸ“Š çµ±è¨ˆæƒ…å ±:\")\nprint(f\"- ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files_info)}\")\nprint(f\"- ç·ã‚µã‚¤ã‚º: {sum(f['size'] for f in files_info) / 1024:.1f} KB\")\n\n# Check each critical file\nprint(f\"\\nğŸ” é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãƒã‚§ãƒƒã‚¯:\")\nprint(\"-\" * 40)\n\n# 1. Check streamlit_app.py\nstreamlit_app_path = os.path.join(output_dir, 'streamlit_app.py')\nif os.path.exists(streamlit_app_path):\n    with open(streamlit_app_path, 'r', encoding='utf-8') as f:\n        content = f.read()\n    \n    print(\"âœ… streamlit_app.py - ä¸»è¦æ©Ÿèƒ½ãƒã‚§ãƒƒã‚¯:\")\n    \n    # Check for key functions and features\n    features_check = {\n        'Streamlit imports': 'import streamlit as st' in content,\n        'Web scraping imports': 'from bs4 import BeautifulSoup' in content and 'import requests' in content,\n        'Data processing': 'import pandas as pd' in content,\n        'Page configuration': 'st.set_page_config' in content,\n        'CSS styling': 'st.markdown(\"\"\"' in content and '<style>' in content,\n        'Input fields': 'st.sidebar.text_input' in content,\n        'Scraping function': 'def scrape_data(' in content,\n        'Progress bar': 'st.progress(' in content,\n        'Error handling': 'try:' in content and 'except' in content,\n        'Random delay': 'random.uniform(' in content and 'time.sleep(' in content,\n        'User-Agent header': 'User-Agent' in content,\n        'Data preview': 'st.dataframe(' in content,\n        'Download buttons': 'st.download_button(' in content,\n        'Excel export': 'to_excel(' in content,\n        'CSV export': 'to_csv(' in content,\n        'Session state': 'st.session_state' in content,\n        'CSS selector guide': 'CSSã‚»ãƒ¬ã‚¯ã‚¿ã®å–å¾—æ–¹æ³•' in content,\n        'Warning messages': 'é‡è¦ãªæ³¨æ„äº‹é …' in content\n    }\n    \n    for feature, exists in features_check.items():\n        status = \"âœ…\" if exists else \"âŒ\"\n        print(f\"  {status} {feature}\")\n    \n    # Count lines of code\n    lines = content.split('\\n')\n    print(f\"  ğŸ“ ç·è¡Œæ•°: {len(lines)} è¡Œ\")\n    print(f\"  ğŸ“ ã‚³ãƒ¡ãƒ³ãƒˆè¡Œæ•°: {sum(1 for line in lines if line.strip().startswith('#'))} è¡Œ\")\n    \nelse:\n    print(\"âŒ streamlit_app.py ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n\n# 2. Check requirements.txt\nrequirements_path = os.path.join(output_dir, 'requirements.txt')\nif os.path.exists(requirements_path):\n    with open(requirements_path, 'r', encoding='utf-8') as f:\n        requirements = f.read().strip().split('\\n')\n    \n    print(f\"\\nâœ… requirements.txt - ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯:\")\n    required_packages = ['streamlit', 'requests', 'beautifulsoup4', 'pandas', 'openpyxl', 'lxml']\n    \n    for package in required_packages:\n        found = any(package in req for req in requirements)\n        status = \"âœ…\" if found else \"âŒ\"\n        print(f\"  {status} {package}\")\n    \n    print(f\"  ğŸ“¦ ç·ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ•°: {len([r for r in requirements if r.strip()])} å€‹\")\nelse:\n    print(\"âŒ requirements.txt ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n\n# 3. Check README.md\nreadme_path = os.path.join(output_dir, 'README.md')\nif os.path.exists(readme_path):\n    with open(readme_path, 'r', encoding='utf-8') as f:\n        readme_content = f.read()\n    \n    print(f\"\\nâœ… README.md - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ãƒã‚§ãƒƒã‚¯:\")\n    \n    readme_sections = {\n        'æ©Ÿèƒ½èª¬æ˜': '## ğŸš€ æ©Ÿèƒ½' in readme_content,\n        'ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †': '## ğŸ› ï¸ ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«' in readme_content,\n        'ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †': '## â˜ï¸ Streamlit Cloudã§ã®ãƒ‡ãƒ—ãƒ­ã‚¤' in readme_content,\n        'ä½¿ç”¨æ–¹æ³•': '## ğŸ“– ä½¿ç”¨æ–¹æ³•' in readme_content,\n        'CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚¬ã‚¤ãƒ‰': 'CSSã‚»ãƒ¬ã‚¯ã‚¿ã®å–å¾—æ–¹æ³•' in readme_content,\n        'æ³¨æ„äº‹é …': '## âš ï¸ é‡è¦ãªæ³¨æ„äº‹é …' in readme_content,\n        'ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°': '## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°' in readme_content,\n        'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ': '## ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ' in readme_content\n    }\n    \n    for section, exists in readme_sections.items():\n        status = \"âœ…\" if exists else \"âŒ\"\n        print(f\"  {status} {section}\")\n    \n    word_count = len(readme_content.split())\n    print(f\"  ğŸ“„ ç·æ–‡å­—æ•°: {len(readme_content)} æ–‡å­—\")\n    print(f\"  ğŸ“ ç·å˜èªæ•°: {word_count} èª\")\nelse:\n    print(\"âŒ README.md ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n\n# 4. Check .streamlit/config.toml\nconfig_path = os.path.join(output_dir, '.streamlit', 'config.toml')\nif os.path.exists(config_path):\n    with open(config_path, 'r', encoding='utf-8') as f:\n        config_content = f.read()\n    \n    print(f\"\\nâœ… .streamlit/config.toml - è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯:\")\n    \n    config_sections = {\n        'Globalè¨­å®š': '[global]' in config_content,\n        'Serverè¨­å®š': '[server]' in config_content,\n        'Browserè¨­å®š': '[browser]' in config_content,\n        'Themeè¨­å®š': '[theme]' in config_content,\n        'Clientè¨­å®š': '[client]' in config_content,\n        'Runnerè¨­å®š': '[runner]' in config_content,\n        'Loggerè¨­å®š': '[logger]' in config_content\n    }\n    \n    for section, exists in config_sections.items():\n        status = \"âœ…\" if exists else \"âŒ\"\n        print(f\"  {status} {section}\")\nelse:\n    print(\"âŒ .streamlit/config.toml ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n\n# 5. Check other files\nother_files = ['.gitignore', 'setup.sh']\nfor file_name in other_files:\n    file_path = os.path.join(output_dir, file_name)\n    if os.path.exists(file_path):\n        print(f\"\\nâœ… {file_name} - å­˜åœ¨ç¢ºèª: OK\")\n    else:\n        print(f\"\\nâŒ {file_name} - è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n\n# Final validation\nprint(f\"\\nğŸ¯ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Œå…¨æ€§è©•ä¾¡:\")\nprint(\"-\" * 30)\n\nessential_files = [\n    'streamlit_app.py',\n    'requirements.txt', \n    'README.md',\n    '.streamlit/config.toml',\n    '.gitignore'\n]\n\nmissing_files = []\nfor file_name in essential_files:\n    file_path = os.path.join(output_dir, file_name)\n    if not os.path.exists(file_path):\n        missing_files.append(file_name)\n\nif not missing_files:\n    print(\"âœ… ã™ã¹ã¦ã®å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã™\")\n    print(\"âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯å®Œå…¨ã§ã™\")\n    \n    print(f\"\\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\")\n    print(\"1. ãƒ­ãƒ¼ã‚«ãƒ«ã§ãƒ†ã‚¹ãƒˆ:\")\n    print(\"   cd /home/user/output\")\n    print(\"   pip install -r requirements.txt\")\n    print(\"   streamlit run streamlit_app.py\")\n    print(\"\")\n    print(\"2. GitHubã«ãƒ—ãƒƒã‚·ãƒ¥:\")\n    print(\"   git init\")\n    print(\"   git add .\")\n    print(\"   git commit -m 'Initial commit: Streamlit web scraping tool'\")\n    print(\"   git remote add origin <your-repo-url>\")\n    print(\"   git push -u origin main\")\n    print(\"\")\n    print(\"3. Streamlit Cloudã§ãƒ‡ãƒ—ãƒ­ã‚¤:\")\n    print(\"   https://share.streamlit.io/ ã§ãƒªãƒã‚¸ãƒˆãƒªã‚’é¸æŠã—ã¦ãƒ‡ãƒ—ãƒ­ã‚¤\")\n    \nelse:\n    print(f\"âŒ ä¸è¶³ã—ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(missing_files)}\")\n\nprint(f\"\\nğŸ“‹ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦:\")\nprint(f\"- åå‰: å®‰å…¨ãªWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«\")\nprint(f\"- ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯: Streamlit\")\nprint(f\"- ä¸»è¦æ©Ÿèƒ½: Web scraping, ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–, ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\")\nprint(f\"- ãƒ‡ãƒ—ãƒ­ã‚¤å¯¾å¿œ: Streamlit Cloud ready\")\nprint(f\"- ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: æ•™è‚²ãƒ»ç ”ç©¶ç›®çš„\")\n\n# Create a project summary file\nsummary = {\n    \"project_name\": \"å®‰å…¨ãªWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«\",\n    \"framework\": \"Streamlit\",\n    \"files_created\": len(files_info),\n    \"total_size_kb\": round(sum(f['size'] for f in files_info) / 1024, 1),\n    \"files\": [f['path'] for f in files_info],\n    \"features\": list(features_check.keys()),\n    \"deployment_ready\": len(missing_files) == 0,\n    \"created_at\": \"2024\"\n}\n\nwith open(os.path.join(output_dir, 'project_summary.json'), 'w', encoding='utf-8') as f:\n    json.dump(summary, f, ensure_ascii=False, indent=2)\n\nprint(f\"\\nğŸ’¾ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦ã‚’ project_summary.json ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯\n==================================================\n\nğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ (/home/user/output):\n------------------------------\nâ”œâ”€â”€ .gitignore (2.3 KB)\nâ”œâ”€â”€ .streamlit/config.toml (1.1 KB)\nâ”œâ”€â”€ README.md (9.1 KB)\nâ”œâ”€â”€ requirements.txt (0.1 KB)\nâ”œâ”€â”€ setup.sh (1.3 KB)\nâ”œâ”€â”€ streamlit_app.py (10.0 KB)\n\nğŸ“Š çµ±è¨ˆæƒ…å ±:\n- ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 6\n- ç·ã‚µã‚¤ã‚º: 23.9 KB\n\nğŸ” é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãƒã‚§ãƒƒã‚¯:\n----------------------------------------\nâœ… streamlit_app.py - ä¸»è¦æ©Ÿèƒ½ãƒã‚§ãƒƒã‚¯:\n  âœ… Streamlit imports\n  âœ… Web scraping imports\n  âœ… Data processing\n  âœ… Page configuration\n  âœ… CSS styling\n  âœ… Input fields\n  âœ… Scraping function\n  âœ… Progress bar\n  âœ… Error handling\n  âœ… Random delay\n  âœ… User-Agent header\n  âœ… Data preview\n  âœ… Download buttons\n  âœ… Excel export\n  âœ… CSV export\n  âœ… Session state\n  âœ… CSS selector guide\n  âœ… Warning messages\n  ğŸ“ ç·è¡Œæ•°: 293 è¡Œ\n  ğŸ“ ã‚³ãƒ¡ãƒ³ãƒˆè¡Œæ•°: 29 è¡Œ\n\nâœ… requirements.txt - ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯:\n  âœ… streamlit\n  âœ… requests\n  âœ… beautifulsoup4\n  âœ… pandas\n  âœ… openpyxl\n  âœ… lxml\n  ğŸ“¦ ç·ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ•°: 6 å€‹\n\nâœ… README.md - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ãƒã‚§ãƒƒã‚¯:\n  âœ… æ©Ÿèƒ½èª¬æ˜\n  âœ… ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †\n  âœ… ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †\n  âœ… ä½¿ç”¨æ–¹æ³•\n  âœ… CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚¬ã‚¤ãƒ‰\n  âœ… æ³¨æ„äº‹é …\n  âœ… ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n  âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ\n  ğŸ“„ ç·æ–‡å­—æ•°: 5097 æ–‡å­—\n  ğŸ“ ç·å˜èªæ•°: 501 èª\n\nâœ… .streamlit/config.toml - è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯:\n  âœ… Globalè¨­å®š\n  âœ… Serverè¨­å®š\n  âœ… Browserè¨­å®š\n  âœ… Themeè¨­å®š\n  âœ… Clientè¨­å®š\n  âœ… Runnerè¨­å®š\n  âœ… Loggerè¨­å®š\n\nâœ… .gitignore - å­˜åœ¨ç¢ºèª: OK\n\nâœ… setup.sh - å­˜åœ¨ç¢ºèª: OK\n\nğŸ¯ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Œå…¨æ€§è©•ä¾¡:\n------------------------------\nâœ… ã™ã¹ã¦ã®å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã™\nâœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯å®Œå…¨ã§ã™\n\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\n1. ãƒ­ãƒ¼ã‚«ãƒ«ã§ãƒ†ã‚¹ãƒˆ:\n   cd /home/user/output\n   pip install -r requirements.txt\n   streamlit run streamlit_app.py\n\n2. GitHubã«ãƒ—ãƒƒã‚·ãƒ¥:\n   git init\n   git add .\n   git commit -m 'Initial commit: Streamlit web scraping tool'\n   git remote add origin <your-repo-url>\n   git push -u origin main\n\n3. Streamlit Cloudã§ãƒ‡ãƒ—ãƒ­ã‚¤:\n   https://share.streamlit.io/ ã§ãƒªãƒã‚¸ãƒˆãƒªã‚’é¸æŠã—ã¦ãƒ‡ãƒ—ãƒ­ã‚¤\n\nğŸ“‹ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦:\n- åå‰: å®‰å…¨ãªWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«\n- ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯: Streamlit\n- ä¸»è¦æ©Ÿèƒ½: Web scraping, ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–, ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n- ãƒ‡ãƒ—ãƒ­ã‚¤å¯¾å¿œ: Streamlit Cloud ready\n- ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: æ•™è‚²ãƒ»ç ”ç©¶ç›®çš„\n\nğŸ’¾ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦ã‚’ project_summary.json ã«ä¿å­˜ã—ã¾ã—ãŸ\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ğŸ‰ **Streamlit å®‰å…¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«å®Œæˆï¼**\n\nã™ã¹ã¦ã®è¦ä»¶ã‚’æº€ãŸã—ãŸå®Œå…¨ãªStreamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚\n\n### ğŸ“ **ä½œæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ**\n\n```\nweb-scraping-tool/\nâ”œâ”€â”€ streamlit_app.py          # ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ (10.0KB, 293è¡Œ)\nâ”œâ”€â”€ requirements.txt          # Pythonä¾å­˜é–¢ä¿‚ (6ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸)\nâ”œâ”€â”€ README.md                # å®Œå…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ (9.1KB)\nâ”œâ”€â”€ setup.sh                 # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (1.3KB)\nâ”œâ”€â”€ .gitignore              # Gité™¤å¤–ãƒ•ã‚¡ã‚¤ãƒ« (2.3KB)\nâ”œâ”€â”€ .streamlit/config.toml  # Streamlitè¨­å®š (1.1KB)\nâ””â”€â”€ project_summary.json    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦\n```\n\n### âœ… **å®Ÿè£…ã•ã‚ŒãŸæ©Ÿèƒ½**\n\n#### **ğŸ”§ ã‚³ã‚¢æ©Ÿèƒ½**\n- âœ… ãƒ™ãƒ¼ã‚¹URLãƒ»CSSã‚»ãƒ¬ã‚¯ã‚¿ãƒ»ãƒšãƒ¼ã‚¸ç¯„å›²ã®å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰\n- âœ… 1-5ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿæ™‚é–“ï¼ˆæœ€ä½1ç§’ä¿è¨¼ï¼‰\n- âœ… é©åˆ‡ãªUser-Agentè¨­å®š\n- âœ… å …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°\n\n#### **ğŸ‘¥ ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£**\n- âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—è¡¨ç¤º\n- âœ… ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º\n- âœ… Excel/CSV/ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n- âœ… CSSã‚»ãƒ¬ã‚¯ã‚¿å–å¾—ã‚¬ã‚¤ãƒ‰\n\n#### **ğŸ›¡ï¸ å®‰å…¨æ©Ÿèƒ½**\n- âœ… ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿ\n- âœ… ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š\n- âœ… åˆ©ç”¨è¦ç´„ãƒ»robots.txtè­¦å‘Šè¡¨ç¤º\n- âœ… ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®ç¶™ç¶šå‡¦ç†\n\n#### **â˜ï¸ ãƒ‡ãƒ—ãƒ­ã‚¤å¯¾å¿œ**\n- âœ… Streamlit Cloudæœ€é©åŒ–è¨­å®š\n- âœ… å®Œå…¨ãªGitHubé€£æºæº–å‚™\n- âœ… è©³ç´°ãªãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †æ›¸\n\n### ğŸš€ **ã™ãã«ä½¿ãˆã‚‹æ‰‹é †**\n\n#### **1. ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆ**\n```bash\ncd /home/user/output\npip install -r requirements.txt\nstreamlit run streamlit_app.py\n```\n\n#### **2. GitHubãƒ—ãƒƒã‚·ãƒ¥**\n```bash\ngit init\ngit add .\ngit commit -m \"Initial commit: Streamlit web scraping tool\"\ngit remote add origin <your-repo-url>\ngit push -u origin main\n```\n\n#### **3. Streamlit Cloudãƒ‡ãƒ—ãƒ­ã‚¤**\n1. [https://share.streamlit.io/](https://share.streamlit.io/) ã«ã‚¢ã‚¯ã‚»ã‚¹\n2. GitHubãƒªãƒã‚¸ãƒˆãƒªã‚’é¸æŠ\n3. `streamlit_app.py` ã‚’æŒ‡å®šã—ã¦ãƒ‡ãƒ—ãƒ­ã‚¤\n\n### ğŸ“§ **ãƒ¡ãƒ¼ãƒ«å…±æœ‰æ–¹æ³•**\n\nãƒ‡ãƒ—ãƒ­ã‚¤å¾Œã€ç”Ÿæˆã•ã‚Œã‚‹URLï¼ˆä¾‹ï¼š`https://username-web-scraping-tool-streamlit-app-xyz.streamlit.app/`ï¼‰ã‚’ãƒ¡ãƒ¼ãƒ«ã§å…±æœ‰ã™ã‚‹ã ã‘ã§ã™ï¼\n\n### ğŸ’¡ **ç‰¹å¾´**\n\n- **åˆå¿ƒè€…ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼**: CSSã‚»ãƒ¬ã‚¯ã‚¿ã®å–å¾—æ–¹æ³•ã‚’è©³ã—ãèª¬æ˜\n- **å®‰å…¨è¨­è¨ˆ**: æ”»æ’ƒã¨å‹˜é•ã„ã•ã‚Œãªã„é©åˆ‡ãªå¾…æ©Ÿæ™‚é–“\n- **å®Ÿç”¨çš„**: WalkerPlusãªã©å®Ÿéš›ã®ã‚µã‚¤ãƒˆã«å¯¾å¿œ\n- **æ‹¡å¼µå¯èƒ½**: ä»–ã®ã‚µã‚¤ãƒˆã«ã‚‚ç°¡å˜ã«å¯¾å¿œå¯èƒ½\n\nã“ã‚Œã§èª°ã§ã‚‚ç°¡å˜ã«ã€å®‰å…¨ã«Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒã§ãã‚‹ãƒ„ãƒ¼ãƒ«ãŒå®Œæˆã—ã¾ã—ãŸï¼ğŸ¯"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}