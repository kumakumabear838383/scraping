import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from urllib.parse import urljoin, urlparse
import re

def scrape_data(base_url, css_selector, num_pages, page_wait_min, page_wait_max, item_wait_min, item_wait_max):
    """WalkerPluså°‚ç”¨ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°"""
    all_data = []

    for page_num in range(1, num_pages + 1):
        # WalkerPlusç”¨ã®URLç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
        if page_num == 1:
            url = base_url
        else:
            # ãƒ™ãƒ¼ã‚¹URLã®æœ«å°¾ãŒ.htmlã§çµ‚ã‚ã£ã¦ã„ã‚‹å ´åˆã®å‡¦ç†
            if base_url.endswith('.html'):
                url = base_url.replace('.html', f'{page_num}.html')
            else:
                url = f"{base_url.rstrip('/')}/{page_num}.html"

        st.write(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page_num}: {url}")

        try:
            response = requests.get(url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')

            # CSSã‚»ãƒ¬ã‚¯ã‚¿ã§ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—
            items = soup.select(css_selector)

            if not items:
                st.warning(f"ãƒšãƒ¼ã‚¸ {page_num} ã§ã‚¢ã‚¤ãƒ†ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                continue

            st.write(f"âœ… {len(items)} ä»¶ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ç™ºè¦‹")

            # å„ã‚¢ã‚¤ãƒ†ãƒ ã‚’å‡¦ç†
            for i, item in enumerate(items, 1):
                try:
                    # ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒªãƒ³ã‚¯ã‚’å–å¾—
                    text = item.get_text(strip=True)
                    link = item.get('href', '')

                    # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                    if link and not link.startswith('http'):
                        link = urljoin(url, link)

                    all_data.append({
                        'ãƒšãƒ¼ã‚¸': page_num,
                        'ã‚¢ã‚¤ãƒ†ãƒ ç•ªå·': i,
                        'ãƒ†ã‚­ã‚¹ãƒˆ': text,
                        'ãƒªãƒ³ã‚¯': link
                    })

                    # ã‚¢ã‚¤ãƒ†ãƒ é–“å¾…æ©Ÿæ™‚é–“
                    if i < len(items):  # æœ€å¾Œã®ã‚¢ã‚¤ãƒ†ãƒ ä»¥å¤–
                        wait_time = random.uniform(item_wait_min, item_wait_max)
                        time.sleep(wait_time)

                except Exception as e:
                    st.error(f"ã‚¢ã‚¤ãƒ†ãƒ  {i} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    continue

            # ãƒšãƒ¼ã‚¸é–“å¾…æ©Ÿæ™‚é–“
            if page_num < num_pages:
                wait_time = random.uniform(page_wait_min, page_wait_max)
                st.write(f"â³ {wait_time:.1f}ç§’å¾…æ©Ÿä¸­...")
                time.sleep(wait_time)

        except requests.RequestException as e:
            st.error(f"ãƒšãƒ¼ã‚¸ {page_num} ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            continue
        except Exception as e:
            st.error(f"ãƒšãƒ¼ã‚¸ {page_num} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            continue

    return all_data

def main():
    st.title("ğŸš¶ WalkerPlus ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«")
    st.write("WalkerPluså°‚ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã§ã™")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    st.sidebar.header("âš™ï¸ è¨­å®š")

    # ãƒ™ãƒ¼ã‚¹URL
    base_url = st.sidebar.text_input(
        "ãƒ™ãƒ¼ã‚¹URL",
        value="https://www.walkerplus.com/event_list/today/ar0313/",
        help="ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®ãƒ™ãƒ¼ã‚¹URL"
    )

    # CSSã‚»ãƒ¬ã‚¯ã‚¿
    css_selector = st.sidebar.text_input(
        "CSSã‚»ãƒ¬ã‚¯ã‚¿",
        value="a.m-mainlist-item__ttl",
        help="å–å¾—ã—ãŸã„è¦ç´ ã®CSSã‚»ãƒ¬ã‚¯ã‚¿"
    )

    # ãƒšãƒ¼ã‚¸æ•°
    num_pages = st.sidebar.number_input(
        "ãƒšãƒ¼ã‚¸æ•°",
        min_value=1,
        max_value=50,
        value=3,
        help="ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°"
    )

    # ãƒšãƒ¼ã‚¸é–“å¾…æ©Ÿæ™‚é–“
    st.sidebar.subheader("â±ï¸ ãƒšãƒ¼ã‚¸é–“å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰")
    page_wait_min = st.sidebar.number_input(
        "æœ€å°",
        min_value=0.1,
        max_value=10.0,
        value=1.0,
        step=0.1,
        key="page_wait_min"
    )
    page_wait_max = st.sidebar.number_input(
        "æœ€å¤§",
        min_value=0.1,
        max_value=10.0,
        value=3.0,
        step=0.1,
        key="page_wait_max"
    )

    # ã‚¢ã‚¤ãƒ†ãƒ é–“å¾…æ©Ÿæ™‚é–“
    st.sidebar.subheader("â±ï¸ ã‚¢ã‚¤ãƒ†ãƒ é–“å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰")
    item_wait_min = st.sidebar.number_input(
        "æœ€å°",
        min_value=0.0,
        max_value=5.0,
        value=0.2,
        step=0.1,
        key="item_wait_min"
    )
    item_wait_max = st.sidebar.number_input(
        "æœ€å¤§",
        min_value=0.0,
        max_value=5.0,
        value=0.5,
        step=0.1,
        key="item_wait_max"
    )

    # è¨­å®šç¢ºèª
    if page_wait_min > page_wait_max:
        st.sidebar.error("ãƒšãƒ¼ã‚¸é–“å¾…æ©Ÿæ™‚é–“: æœ€å°å€¤ãŒæœ€å¤§å€¤ã‚ˆã‚Šå¤§ãã„ã§ã™")
        return

    if item_wait_min > item_wait_max:
        st.sidebar.error("ã‚¢ã‚¤ãƒ†ãƒ é–“å¾…æ©Ÿæ™‚é–“: æœ€å°å€¤ãŒæœ€å¤§å€¤ã‚ˆã‚Šå¤§ãã„ã§ã™")
        return

    # ç¾åœ¨ã®è¨­å®šã‚’è¡¨ç¤º
    st.subheader("ğŸ“‹ ç¾åœ¨ã®è¨­å®š")
    col1, col2 = st.columns(2)

    with col1:
        st.write(f"**ãƒ™ãƒ¼ã‚¹URL:** {base_url}")
        st.write(f"**CSSã‚»ãƒ¬ã‚¯ã‚¿:** `{css_selector}`")
        st.write(f"**ãƒšãƒ¼ã‚¸æ•°:** {num_pages}")

    with col2:
        st.write(f"**ãƒšãƒ¼ã‚¸é–“å¾…æ©Ÿ:** {page_wait_min}-{page_wait_max}ç§’")
        st.write(f"**ã‚¢ã‚¤ãƒ†ãƒ é–“å¾…æ©Ÿ:** {item_wait_min}-{item_wait_max}ç§’")

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    if st.button("ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹", type="primary"):
        if not base_url:
            st.error("ãƒ™ãƒ¼ã‚¹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return

        if not css_selector:
            st.error("CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
        progress_bar = st.progress(0)
        status_text = st.empty()

        start_time = time.time()

        try:
            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
            data = scrape_data(
                base_url, css_selector, num_pages,
                page_wait_min, page_wait_max,
                item_wait_min, item_wait_max
            )

            progress_bar.progress(100)
            end_time = time.time()

            if data:
                st.success(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼ {len(data)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
                st.write(f"â±ï¸ å®Ÿè¡Œæ™‚é–“: {end_time - start_time:.1f}ç§’")

                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
                df = pd.DataFrame(data)

                # çµæœè¡¨ç¤º
                st.subheader("ğŸ“Š å–å¾—ãƒ‡ãƒ¼ã‚¿")
                st.dataframe(df, use_container_width=True)

                # çµ±è¨ˆæƒ…å ±
                st.subheader("ğŸ“ˆ çµ±è¨ˆæƒ…å ±")
                col1, col2, col3 = st.columns(3)

                with col1:
                    st.metric("ç·ã‚¢ã‚¤ãƒ†ãƒ æ•°", len(df))

                with col2:
                    st.metric("å‡¦ç†ãƒšãƒ¼ã‚¸æ•°", df['ãƒšãƒ¼ã‚¸'].nunique())

                with col3:
                    avg_items = len(df) / df['ãƒšãƒ¼ã‚¸'].nunique()
                    st.metric("å¹³å‡ã‚¢ã‚¤ãƒ†ãƒ æ•°/ãƒšãƒ¼ã‚¸", f"{avg_items:.1f}")

                # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                csv = df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv,
                    file_name=f"walkerplus_data_{int(time.time())}.csv",
                    mime="text/csv"
                )

            else:
                st.warning("ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

        finally:
            progress_bar.empty()
            status_text.empty()

if __name__ == "__main__":
    main()
