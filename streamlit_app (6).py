import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import re
import time

def try_decode_content(content):
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡å­—ã‚³ãƒ¼ãƒ‰å‡¦ç†
    UTF-8 -> Shift_JIS -> EUC-JP ã®é †ã§è©¦è¡Œ
    """
    encodings = ['utf-8', 'shift_jis', 'euc-jp']

    for encoding in encodings:
        try:
            return content.decode(encoding)
        except (UnicodeDecodeError, AttributeError):
            continue

    # ã™ã¹ã¦å¤±æ•—ã—ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–ã—ã¦ãƒ‡ã‚³ãƒ¼ãƒ‰
    try:
        return content.decode('utf-8', errors='ignore')
    except:
        return str(content)

def get_page_content(url):
    """
    ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        # ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡å­—ã‚³ãƒ¼ãƒ‰å‡¦ç†
        if isinstance(response.content, bytes):
            html_content = try_decode_content(response.content)
        else:
            html_content = response.content

        return html_content

    except requests.exceptions.RequestException as e:
        st.error(f"ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        return None
    except Exception as e:
        st.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None

def extract_event_info(soup):
    """
    ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡º
    """
    events = []

    try:
        # WalkerPlusã®ä¸€èˆ¬çš„ãªã‚¤ãƒ™ãƒ³ãƒˆè¦ç´ ã‚’æ¤œç´¢
        event_selectors = [
            '.item',
            '.event-item',
            '.list-item',
            'article',
            '.content-item'
        ]

        event_elements = []
        for selector in event_selectors:
            elements = soup.select(selector)
            if elements:
                event_elements = elements
                break

        if not event_elements:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚ˆã‚Šåºƒç¯„å›²ãªæ¤œç´¢
            event_elements = soup.find_all(['div', 'article'], class_=re.compile(r'(item|event|list)', re.I))

        for element in event_elements[:50]:  # æœ€å¤§50ä»¶ã«åˆ¶é™
            try:
                # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
                title_selectors = ['h2', 'h3', 'h4', '.title', '.name', 'a']
                title = None

                for selector in title_selectors:
                    title_elem = element.select_one(selector)
                    if title_elem and title_elem.get_text(strip=True):
                        title = title_elem.get_text(strip=True)
                        break

                if not title:
                    continue

                # æ—¥ä»˜æŠ½å‡º
                date_text = ""
                date_selectors = ['.date', '.time', '.period', '.schedule']
                for selector in date_selectors:
                    date_elem = element.select_one(selector)
                    if date_elem:
                        date_text = date_elem.get_text(strip=True)
                        break

                # å ´æ‰€æŠ½å‡º
                location = ""
                location_selectors = ['.place', '.location', '.venue', '.address']
                for selector in location_selectors:
                    location_elem = element.select_one(selector)
                    if location_elem:
                        location = location_elem.get_text(strip=True)
                        break

                # URLæŠ½å‡º
                url = ""
                link_elem = element.select_one('a')
                if link_elem and link_elem.get('href'):
                    href = link_elem.get('href')
                    if href.startswith('http'):
                        url = href
                    elif href.startswith('/'):
                        url = f"https://www.walkerplus.com{href}"

                # èª¬æ˜æŠ½å‡º
                description = ""
                desc_selectors = ['.description', '.summary', '.text', 'p']
                for selector in desc_selectors:
                    desc_elem = element.select_one(selector)
                    if desc_elem:
                        desc_text = desc_elem.get_text(strip=True)
                        if len(desc_text) > 20:  # ååˆ†ãªé•·ã•ã®èª¬æ˜ã®ã¿
                            description = desc_text[:200]  # 200æ–‡å­—ã«åˆ¶é™
                            break

                events.append({
                    'ã‚¿ã‚¤ãƒˆãƒ«': title,
                    'æ—¥ä»˜': date_text,
                    'å ´æ‰€': location,
                    'URL': url,
                    'èª¬æ˜': description
                })

            except Exception as e:
                # å€‹åˆ¥ã®ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ã—ã¦ç¶šè¡Œ
                continue

    except Exception as e:
        st.error(f"ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã®æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

    return events

def main():
    st.set_page_config(
        page_title="WalkerPlus ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±å–å¾—",
        page_icon="ğŸ‰",
        layout="wide"
    )

    st.title("ğŸ‰ WalkerPlus ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±å–å¾—ãƒ„ãƒ¼ãƒ«")
    st.markdown("---")

    # URLå…¥åŠ›
    st.subheader("ğŸ“ URLå…¥åŠ›")
    url = st.text_input(
        "WalkerPlusã®ã‚¤ãƒ™ãƒ³ãƒˆãƒšãƒ¼ã‚¸URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
        placeholder="https://www.walkerplus.com/..."
    )

    # å®Ÿè¡Œãƒœã‚¿ãƒ³
    if st.button("ğŸ” ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—", type="primary"):
        if not url:
            st.warning("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        if "walkerplus.com" not in url:
            st.warning("WalkerPlusã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼è¡¨ç¤º
        progress_bar = st.progress(0)
        status_text = st.empty()

        try:
            # ãƒšãƒ¼ã‚¸å–å¾—
            status_text.text("ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")
            progress_bar.progress(25)

            html_content = get_page_content(url)
            if not html_content:
                return

            # HTMLè§£æ
            status_text.text("HTMLã‚’è§£æä¸­...")
            progress_bar.progress(50)

            soup = BeautifulSoup(html_content, 'html.parser')

            # ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±æŠ½å‡º
            status_text.text("ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡ºä¸­...")
            progress_bar.progress(75)

            events = extract_event_info(soup)

            progress_bar.progress(100)
            status_text.text("å®Œäº†!")

            time.sleep(0.5)
            progress_bar.empty()
            status_text.empty()

            if not events:
                st.warning("ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚URLã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                return

            # çµæœè¡¨ç¤º
            st.success(f"âœ… {len(events)}ä»¶ã®ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸï¼")

            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
            df = pd.DataFrame(events)

            # çµæœè¡¨ç¤º
            st.subheader("ğŸ“Š å–å¾—çµæœ")
            st.dataframe(df, use_container_width=True)

            # Excelå‡ºåŠ›
            st.subheader("ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›")

            # Excelãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"walkerplus_events_{timestamp}.xlsx"

            try:
                df.to_excel(filename, index=False, engine='openpyxl')

                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                with open(filename, 'rb') as f:
                    st.download_button(
                        label="ğŸ“¥ Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=f.read(),
                        file_name=filename,
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )

                st.success("Excelãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¾ã—ãŸï¼")

            except Exception as e:
                st.error(f"Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

        except Exception as e:
            st.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            progress_bar.empty()
            status_text.empty()

    # ä½¿ç”¨æ–¹æ³•
    with st.expander("ğŸ“– ä½¿ç”¨æ–¹æ³•"):
        st.markdown("""
        ### ä½¿ç”¨æ‰‹é †
        1. **URLå…¥åŠ›**: WalkerPlusã®ã‚¤ãƒ™ãƒ³ãƒˆä¸€è¦§ãƒšãƒ¼ã‚¸ã®URLã‚’å…¥åŠ›
        2. **å–å¾—å®Ÿè¡Œ**: ã€Œã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
        3. **çµæœç¢ºèª**: å–å¾—ã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’ç¢ºèª
        4. **ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›**: å¿…è¦ã«å¿œã˜ã¦Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

        ### å¯¾å¿œURLä¾‹
        - `https://www.walkerplus.com/event_list/today/`
        - `https://www.walkerplus.com/event_list/ar0313/`
        - ãã®ä»–ã®WalkerPlusã‚¤ãƒ™ãƒ³ãƒˆãƒšãƒ¼ã‚¸

        ### æ³¨æ„äº‹é …
        - å–å¾—ã§ãã‚‹æƒ…å ±ã¯ãƒšãƒ¼ã‚¸ã®æ§‹é€ ã«ã‚ˆã£ã¦ç•°ãªã‚Šã¾ã™
        - å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿å–å¾—æ™‚ã¯æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™
        - æ–‡å­—åŒ–ã‘ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€åŸºæœ¬çš„ãªæ–‡å­—ã‚³ãƒ¼ãƒ‰å‡¦ç†ã§å¯¾å¿œã—ã¾ã™
        """)

if __name__ == "__main__":
    main()
