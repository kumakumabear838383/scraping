import streamlit as st
import requests
import pandas as pd
from datetime import datetime
import time

# BeautifulSoupã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
    st.success("âœ… BeautifulSoup successfully imported")
except ImportError as e:
    st.error(f"âŒ BeautifulSoup import failed: {e}")
    st.error("Please check requirements.txt and ensure beautifulsoup4 is installed")
    BS4_AVAILABLE = False
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªHTMLãƒ‘ãƒ¼ã‚¹ã®ã¿
    BeautifulSoup = None

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®è¨­å®š
st.set_page_config(
    page_title="Web Scraping Tool",
    page_icon="ğŸ”",
    layout="wide"
)

def safe_request(url, timeout=10):
    """å®‰å…¨ãªHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        return response
    except requests.exceptions.RequestException as e:
        st.error(f"Request failed: {e}")
        return None

def parse_html_content(html_content):
    """HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è§£æ"""
    if not BS4_AVAILABLE:
        st.warning("BeautifulSoup not available. Using basic text extraction.")
        return {"title": "N/A", "text": html_content[:500] + "..."}

    try:
        soup = BeautifulSoup(html_content, 'html.parser')

        # ã‚¿ã‚¤ãƒˆãƒ«ã®å–å¾—
        title = soup.find('title')
        title_text = title.get_text().strip() if title else "No title found"

        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å–å¾—
        # ä¸€èˆ¬çš„ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚°ã‚’å„ªå…ˆé †ä½ã§æ¤œç´¢
        content_selectors = [
            'main', 'article', '.content', '#content', 
            '.main-content', '.post-content', 'body'
        ]

        main_content = ""
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                main_content = content_elem.get_text(strip=True)
                break

        if not main_content:
            main_content = soup.get_text(strip=True)

        return {
            "title": title_text,
            "text": main_content[:1000] + "..." if len(main_content) > 1000 else main_content,
            "links": len(soup.find_all('a')),
            "images": len(soup.find_all('img'))
        }

    except Exception as e:
        st.error(f"HTML parsing failed: {e}")
        return {"title": "Parse Error", "text": "Failed to parse HTML content"}

# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
def main():
    st.title("ğŸ” Web Scraping Tool")
    st.markdown("---")

    # BeautifulSoupã®çŠ¶æ…‹è¡¨ç¤º
    if BS4_AVAILABLE:
        st.success("ğŸŸ¢ BeautifulSoup is ready")
    else:
        st.error("ğŸ”´ BeautifulSoup is not available")
        st.info("The app will work with limited functionality")

    # URLå…¥åŠ›
    url = st.text_input(
        "Enter URL to scrape:",
        placeholder="https://example.com",
        help="Enter a valid URL to extract content"
    )

    if st.button("ğŸš€ Scrape Content", type="primary"):
        if not url:
            st.warning("Please enter a URL")
            return

        if not url.startswith(('http://', 'https://')):
            st.error("Please enter a valid URL starting with http:// or https://")
            return

        with st.spinner("Fetching content..."):
            # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            response = safe_request(url)

            if response is None:
                return

            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è§£æ
            parsed_content = parse_html_content(response.text)

            # çµæœã®è¡¨ç¤º
            st.success("âœ… Content successfully scraped!")

            col1, col2 = st.columns([2, 1])

            with col1:
                st.subheader("ğŸ“„ Page Content")
                st.write(f"**Title:** {parsed_content['title']}")
                st.text_area("Content Preview:", parsed_content['text'], height=300)

            with col2:
                st.subheader("ğŸ“Š Page Statistics")
                if BS4_AVAILABLE:
                    st.metric("Links Found", parsed_content.get('links', 'N/A'))
                    st.metric("Images Found", parsed_content.get('images', 'N/A'))
                st.metric("Content Length", len(parsed_content['text']))
                st.metric("Response Status", response.status_code)

            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
            st.subheader("ğŸ’¾ Download Results")

            # CSVãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            df = pd.DataFrame([{
                'URL': url,
                'Title': parsed_content['title'],
                'Content_Preview': parsed_content['text'][:200],
                'Scraped_At': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'Status_Code': response.status_code
            }])

            csv_data = df.to_csv(index=False)

            st.download_button(
                label="ğŸ“¥ Download as CSV",
                data=csv_data,
                file_name=f"scraped_content_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )

# ã‚µã‚¤ãƒ‰ãƒãƒ¼æƒ…å ±
with st.sidebar:
    st.header("â„¹ï¸ Information")
    st.info("""
    This tool allows you to scrape web content safely.

    **Features:**
    - Safe HTTP requests
    - HTML content parsing
    - Content extraction
    - CSV export

    **Requirements:**
    - Valid URL
    - Internet connection
    - BeautifulSoup4 library
    """)

    st.header("ğŸ”§ Debug Info")
    st.write(f"BeautifulSoup Available: {BS4_AVAILABLE}")
    st.write(f"Streamlit Version: {st.__version__}")

if __name__ == "__main__":
    main()
