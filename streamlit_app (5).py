import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import chardet
import io
from openpyxl import Workbook
from openpyxl.utils.dataframe import dataframe_to_rows

def detect_encoding(response):
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡º"""
    # ã¾ãšchardetã§è‡ªå‹•æ¤œå‡ºã‚’è©¦è¡Œ
    detected = chardet.detect(response.content)
    detected_encoding = detected.get('encoding', 'utf-8')

    # ä¸€èˆ¬çš„ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ãƒªã‚¹ãƒˆ
    encodings_to_try = [
        detected_encoding,
        'utf-8',
        'shift_jis',
        'euc-jp',
        'iso-2022-jp',
        'cp932'
    ]

    # å„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
    for encoding in encodings_to_try:
        try:
            if encoding:
                response.encoding = encoding
                # ãƒ‡ã‚³ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
                test_text = response.text[:100]
                return encoding, detected.get('confidence', 0)
        except (UnicodeDecodeError, LookupError):
            continue

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    response.encoding = 'utf-8'
    return 'utf-8', 0

def scrape_walkerplus_events(base_url, max_pages, delay_seconds):
    """WalkerPlusã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    all_events = []

    progress_bar = st.progress(0)
    status_text = st.empty()
    encoding_info = st.empty()

    for page in range(1, max_pages + 1):
        status_text.text(f'ãƒšãƒ¼ã‚¸ {page}/{max_pages} ã‚’å‡¦ç†ä¸­...')

        # URLã‚’æ§‹ç¯‰
        if page == 1:
            url = base_url
        else:
            separator = '&' if '?' in base_url else '?'
            url = f"{base_url}{separator}p={page}"

        try:
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡ºãƒ»è¨­å®š
            detected_encoding, confidence = detect_encoding(response)
            encoding_info.text(f'æ¤œå‡ºã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {detected_encoding} (ä¿¡é ¼åº¦: {confidence:.2f})')

            # HTMLã‚’ãƒ‘ãƒ¼ã‚¹
            soup = BeautifulSoup(response.text, 'html.parser')

            # ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡º
            events = soup.find_all('div', class_='eventListItem')

            if not events:
                st.warning(f'ãƒšãƒ¼ã‚¸ {page} ã§ã‚¤ãƒ™ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚')
                break

            for event in events:
                event_data = {}

                # ã‚¿ã‚¤ãƒˆãƒ«
                title_elem = event.find('h3') or event.find('h2') or event.find('a')
                event_data['ã‚¿ã‚¤ãƒˆãƒ«'] = title_elem.get_text(strip=True) if title_elem else 'ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜'

                # æ—¥æ™‚
                date_elem = event.find('time') or event.find('span', class_='date')
                if not date_elem:
                    date_elem = event.find('p', string=lambda text: text and ('æœˆ' in text or 'æ—¥' in text))
                event_data['æ—¥æ™‚'] = date_elem.get_text(strip=True) if date_elem else 'æ—¥æ™‚ä¸æ˜'

                # å ´æ‰€
                venue_elem = event.find('span', class_='venue') or event.find('p', class_='venue')
                if not venue_elem:
                    venue_elem = event.find('span', string=lambda text: text and ('ä¼šå ´' in text or 'å ´æ‰€' in text))
                event_data['å ´æ‰€'] = venue_elem.get_text(strip=True) if venue_elem else 'å ´æ‰€ä¸æ˜'

                # URL
                link_elem = event.find('a')
                if link_elem and link_elem.get('href'):
                    href = link_elem.get('href')
                    if href.startswith('/'):
                        event_data['URL'] = f"https://www.walkerplus.com{href}"
                    else:
                        event_data['URL'] = href
                else:
                    event_data['URL'] = 'URLä¸æ˜'

                all_events.append(event_data)

            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’æ›´æ–°
            progress_bar.progress(page / max_pages)

            # å¾…æ©Ÿæ™‚é–“
            if page < max_pages and delay_seconds > 0:
                time.sleep(delay_seconds)

        except requests.RequestException as e:
            st.error(f'ãƒšãƒ¼ã‚¸ {page} ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}')
            break
        except Exception as e:
            st.error(f'ãƒšãƒ¼ã‚¸ {page} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}')
            break

    progress_bar.progress(1.0)
    status_text.text('ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼')

    return all_events

def create_excel_file(df):
    """DataFrameã‹ã‚‰Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
    output = io.BytesIO()

    # Workbookã‚’ä½œæˆ
    wb = Workbook()
    ws = wb.active
    ws.title = "WalkerPlusã‚¤ãƒ™ãƒ³ãƒˆ"

    # DataFrameã‚’ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿
    for r in dataframe_to_rows(df, index=False, header=True):
        ws.append(r)

    # åˆ—å¹…ã‚’è‡ªå‹•èª¿æ•´
    for column in ws.columns:
        max_length = 0
        column_letter = column[0].column_letter
        for cell in column:
            try:
                if len(str(cell.value)) > max_length:
                    max_length = len(str(cell.value))
            except:
                pass
        adjusted_width = min(max_length + 2, 50)
        ws.column_dimensions[column_letter].width = adjusted_width

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    wb.save(output)
    output.seek(0)

    return output.getvalue()

def main():
    st.set_page_config(
        page_title="WalkerPlus ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°",
        page_icon="ğŸª",
        layout="wide"
    )

    st.title("ğŸª WalkerPlus ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")
    st.markdown("WalkerPlusã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—ã—ã¦CSVãƒ»Excelãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®š
    st.sidebar.header("âš™ï¸ è¨­å®š")

    # URLå…¥åŠ›
    st.sidebar.subheader("ğŸ”— URLè¨­å®š")
    base_url = st.sidebar.text_input(
        "WalkerPlus URL",
        value="https://www.walkerplus.com/event_list/today/",
        help="WalkerPlusã®ã‚¤ãƒ™ãƒ³ãƒˆä¸€è¦§ãƒšãƒ¼ã‚¸ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
    )

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®š
    st.sidebar.subheader("ğŸ“Š ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®š")
    max_pages = st.sidebar.slider("å–å¾—ãƒšãƒ¼ã‚¸æ•°", 1, 20, 3)
    delay_seconds = st.sidebar.slider("ãƒšãƒ¼ã‚¸é–“å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰", 0.0, 5.0, 1.0, 0.5)

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œãƒœã‚¿ãƒ³
    if st.sidebar.button("ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹", type="primary"):
        if not base_url:
            st.error("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        with st.spinner("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­..."):
            events = scrape_walkerplus_events(base_url, max_pages, delay_seconds)

        if events:
            st.success(f"âœ… {len(events)}ä»¶ã®ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸï¼")

            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
            df = pd.DataFrame(events)

            # ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
            st.subheader("ğŸ“‹ å–å¾—ã—ãŸã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±")
            st.dataframe(df, use_container_width=True)

            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
            st.subheader("ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

            col1, col2 = st.columns(2)

            with col1:
                # CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                csv_data = df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ğŸ“„ CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv_data,
                    file_name=f"walkerplus_events_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )

            with col2:
                # Excel ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                excel_data = create_excel_file(df)
                st.download_button(
                    label="ğŸ“Š Excel ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=excel_data,
                    file_name=f"walkerplus_events_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

            # çµ±è¨ˆæƒ…å ±
            st.subheader("ğŸ“ˆ çµ±è¨ˆæƒ…å ±")
            col1, col2, col3 = st.columns(3)

            with col1:
                st.metric("å–å¾—ã‚¤ãƒ™ãƒ³ãƒˆæ•°", len(events))

            with col2:
                unique_venues = df['å ´æ‰€'].nunique()
                st.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯ä¼šå ´æ•°", unique_venues)

            with col3:
                valid_urls = df[df['URL'] != 'URLä¸æ˜'].shape[0]
                st.metric("æœ‰åŠ¹URLæ•°", valid_urls)

        else:
            st.error("âŒ ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚URLã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # ä½¿ç”¨æ–¹æ³•
    with st.expander("ğŸ“– ä½¿ç”¨æ–¹æ³•"):
        st.markdown("""
        ### åŸºæœ¬çš„ãªä½¿ã„æ–¹
        1. **URLè¨­å®š**: WalkerPlusã®ã‚¤ãƒ™ãƒ³ãƒˆä¸€è¦§ãƒšãƒ¼ã‚¸ã®URLã‚’å…¥åŠ›
        2. **è¨­å®šèª¿æ•´**: å–å¾—ãƒšãƒ¼ã‚¸æ•°ã¨å¾…æ©Ÿæ™‚é–“ã‚’èª¿æ•´
        3. **å®Ÿè¡Œ**: ã€Œã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
        4. **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**: CSV ã¾ãŸã¯ Excel ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

        ### æ³¨æ„äº‹é …
        - é©åˆ‡ãªå¾…æ©Ÿæ™‚é–“ã‚’è¨­å®šã—ã¦ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†ã«ã—ã¦ãã ã•ã„
        - å¤§é‡ã®ãƒšãƒ¼ã‚¸ã‚’ä¸€åº¦ã«å–å¾—ã™ã‚‹å ´åˆã¯ã€å¾…æ©Ÿæ™‚é–“ã‚’é•·ã‚ã«è¨­å®šã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™
        - æ–‡å­—åŒ–ã‘ãŒç™ºç”Ÿã—ãŸå ´åˆã€è‡ªå‹•çš„ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡ºãƒ»ä¿®æ­£ã—ã¾ã™

        ### å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼
        - **CSV**: UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆBOMä»˜ãï¼‰ã§å‡ºåŠ›
        - **Excel**: .xlsxå½¢å¼ã§å‡ºåŠ›ã€åˆ—å¹…è‡ªå‹•èª¿æ•´æ©Ÿèƒ½ä»˜ã
        """)

if __name__ == "__main__":
    main()
