# 🏢 WalkerPlus スポット情報スクレイピングツール

WalkerPlusのスポット一覧ページから情報を自動収集するStreamlitアプリケーションです。

## ✨ 特徴

- 🔄 **正確なページネーション対応**: WalkerPlusの実際のURL構造に完全対応
- 🎯 **柔軟なデータ抽出**: 複数のHTML構造パターンに対応
- ⏱️ **サーバー負荷軽減**: ランダム待機時間でアクセス制御
- 📊 **データ可視化**: 取得データの統計情報とプレビュー機能
- 📥 **CSV出力**: 取得したデータをCSV形式でダウンロード
- 🔍 **デバッグ支援**: ページ構造プレビュー機能

## 🚀 クイックスタート

### 1. 環境構築

```bash
# 必要なパッケージをインストール
pip install -r requirements.txt
```

### 2. アプリケーション起動

```bash
# Streamlitアプリを起動
streamlit run streamlit_app.py
```

### 3. ブラウザでアクセス

アプリが起動したら、ブラウザで `http://localhost:8501` にアクセスしてください。

## 📋 使用方法

### 基本的な使い方

1. **ベースURL入力**: WalkerPlusのスポット一覧ページのURLを入力
2. **設定調整**: 最大ページ数と待機時間を設定
3. **プレビュー**: 「ページ構造プレビュー」でデータ構造を確認（推奨）
4. **実行**: 「スクレイピング開始」ボタンをクリック
5. **ダウンロード**: 取得したデータをCSVでダウンロード

### URL構造について

WalkerPlusは以下のページネーション構造を使用しています：

```
1ページ目: https://www.walkerplus.com/spot_list/ar0623/sg0051/
2ページ目: https://www.walkerplus.com/spot_list/ar0623/sg0051/2.html
3ページ目: https://www.walkerplus.com/spot_list/ar0623/sg0051/3.html
Nページ目: https://www.walkerplus.com/spot_list/ar0623/sg0051/N.html
```

**重要**: 1ページ目のみ番号が付かず、2ページ目以降は `{ページ番号}.html` が追加されます。

## 🔧 URL生成ロジック

本ツールは以下のロジックで正確なURLを生成します：

```python
def generate_url(base_url, page_num):
    if page_num == 1:
        # 1ページ目は番号なし
        url = base_url.rstrip('/')
    else:
        # 2ページ目以降はページ番号.htmlを追加
        clean_base_url = base_url.rstrip('/')
        url = f"{clean_base_url}/{page_num}.html"
    return url
```

## 📝 使用例

### 例1: 基本的な使用

```python
# デフォルト設定での使用
base_url = "https://www.walkerplus.com/spot_list/ar0623/sg0051/"
max_pages = 5
delay_range = (1, 3)  # 1-3秒のランダム待機
```

### 例2: カスタム設定

```python
# より多くのページを取得（待機時間も長めに設定）
base_url = "https://www.walkerplus.com/spot_list/ar0623/sg0051/"
max_pages = 10
delay_range = (2, 5)  # 2-5秒のランダム待機
```

### 例3: 他の地域のスポット

```python
# 他の地域のスポット一覧
base_url = "https://www.walkerplus.com/spot_list/ar0613/sg0051/"  # 東京の例
max_pages = 3
delay_range = (1, 2)
```

## 📊 取得データ項目

| 項目 | 説明 |
|------|------|
| タイトル | スポット名 |
| リンク | 詳細ページのURL |
| 住所 | スポットの住所 |
| 説明 | スポットの説明文 |
| カテゴリ | スポットのカテゴリ |
| ページ | 取得元のページ番号 |
| URL | 取得元のページURL |

## ⚙️ 設定項目

### サイドバー設定

- **ベースURL**: スクレイピング対象のWalkerPlusページURL
- **最大ページ数**: 処理するページ数（1-20ページ）
- **最小待機時間**: リクエスト間の最小待機時間（0.5-5.0秒）
- **最大待機時間**: リクエスト間の最大待機時間（1.0-10.0秒）

### 推奨設定

```python
# 一般的な使用
max_pages = 5
delay_range = (1, 3)

# 大量データ取得時
max_pages = 10
delay_range = (2, 5)

# テスト時
max_pages = 2
delay_range = (0.5, 1)
```

## 🔍 デバッグ機能

### ページ構造プレビュー

スクレイピング前にページ構造を確認できます：

1. ベースURLを入力
2. 「ページ構造プレビュー」ボタンをクリック
3. HTML要素の構造を確認
4. 適切なセレクタが使用されているか確認

### エラーハンドリング

- **リクエストエラー**: ネットワークエラーやHTTPエラーを処理
- **解析エラー**: HTML構造の変更に対応
- **部分的成功**: 一部のページでエラーが発生しても継続処理

## 📁 ファイル構成

```
project/
├── streamlit_app.py      # メインアプリケーション
├── requirements.txt      # 依存パッケージ
├── README.md            # このファイル
└── output/              # 出力ファイル保存先
    └── *.csv           # スクレイピング結果
```

## 📦 依存パッケージ

```txt
streamlit>=1.28.0
requests>=2.31.0
beautifulsoup4>=4.12.0
pandas>=2.0.0
lxml>=4.9.0
```

## ⚠️ 注意事項

### 利用規約の遵守

- WalkerPlusの利用規約を必ず確認してください
- robots.txtの内容を尊重してください
- 過度なアクセスは避けてください

### 技術的注意点

- **待機時間**: サーバー負荷軽減のため適切な待機時間を設定
- **User-Agent**: 適切なUser-Agentヘッダーを設定済み
- **セッション管理**: 効率的なHTTPセッションを使用
- **エラー処理**: 堅牢なエラーハンドリングを実装

### データ利用について

- 取得したデータの利用は自己責任で行ってください
- 商用利用の場合は事前に許可を取得してください
- 個人情報が含まれる場合は適切に処理してください

## 🐛 トラブルシューティング

### よくある問題と解決方法

#### 1. データが取得できない

```python
# 解決方法:
# 1. ページ構造プレビューで構造を確認
# 2. URLが正しいか確認
# 3. ネットワーク接続を確認
```

#### 2. 一部のページでエラーが発生

```python
# 正常な動作です
# エラーが発生したページはスキップされ、
# 他のページの処理は継続されます
```

#### 3. 取得データが不完全

```python
# 原因: HTML構造の変更
# 解決方法: ページ構造プレビューで最新の構造を確認
```

## 🔄 更新履歴

### v2.0.0 (最新)
- ✅ **重要**: WalkerPlusの正しいページネーション構造に対応
- ✅ URL生成ロジックの完全修正
- ✅ デフォルトURLの更新
- ✅ より詳細なデバッグ情報
- ✅ エラーハンドリングの改善

### v1.0.0
- 🎉 初回リリース
- 基本的なスクレイピング機能
- CSV出力機能
- Streamlit UI

## 📞 サポート

問題が発生した場合は、以下の情報を含めてお問い合わせください：

1. 使用したURL
2. エラーメッセージ
3. 設定内容
4. ページ構造プレビューの結果

## 📄 ライセンス

このプロジェクトはMITライセンスの下で公開されています。

---

**⚡ 重要な変更点 (v2.0.0)**

このバージョンでは、WalkerPlusの実際のURL構造に合わせて大幅な修正を行いました：

- **修正前**: 全ページで同じURLを使用（誤り）
- **修正後**: 1ページ目は番号なし、2ページ目以降は `{番号}.html` を追加

この修正により、正確なページネーション処理が可能になりました。
