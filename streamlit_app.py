import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from urllib.parse import urljoin, urlparse
import io
from datetime import datetime

# Page configuration
st.set_page_config(
    page_title="å®‰å…¨ãªWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«",
    page_icon="ğŸ•·ï¸",
    layout="wide"
)

# Custom CSS
st.markdown("""
<style>
.main-header {
    font-size: 2.5rem;
    color: #1f77b4;
    text-align: center;
    margin-bottom: 2rem;
}
.section-header {
    font-size: 1.5rem;
    color: #ff7f0e;
    margin-top: 2rem;
    margin-bottom: 1rem;
}
.info-box {
    background-color: #f0f2f6;
    padding: 1rem;
    border-radius: 0.5rem;
    border-left: 4px solid #1f77b4;
    margin: 1rem 0;
}
.warning-box {
    background-color: #fff3cd;
    padding: 1rem;
    border-radius: 0.5rem;
    border-left: 4px solid #ffc107;
    margin: 1rem 0;
}
.success-box {
    background-color: #d4edda;
    padding: 1rem;
    border-radius: 0.5rem;
    border-left: 4px solid #28a745;
    margin: 1rem 0;
}
</style>
""", unsafe_allow_html=True)

# Title
st.markdown('<h1 class="main-header">ğŸ•·ï¸ å®‰å…¨ãªWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«</h1>', unsafe_allow_html=True)

# Warning message
st.markdown("""
<div class="warning-box">
<strong>âš ï¸ é‡è¦ãªæ³¨æ„äº‹é …</strong><br>
ã“ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€å¯¾è±¡ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ã¨robots.txtã‚’å¿…ãšç¢ºèªã—ã¦ãã ã•ã„ã€‚
é©åˆ‡ãªé–“éš”ã§ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’å¿ƒãŒã‘ã€ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†æ³¨æ„ã—ã¦ãã ã•ã„ã€‚
</div>
""", unsafe_allow_html=True)

# Sidebar for settings
st.sidebar.markdown('<h2 class="section-header">âš™ï¸ è¨­å®š</h2>', unsafe_allow_html=True)

# Input fields
base_url = st.sidebar.text_input(
    "ãƒ™ãƒ¼ã‚¹URL",
    value="https://www.walkerplus.com/spot_list/ar0700/sg0107/",
    help="ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®ãƒ™ãƒ¼ã‚¹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
)

css_selector = st.sidebar.text_input(
    "CSSã‚»ãƒ¬ã‚¯ã‚¿",
    value="a.m-mainlist-item__ttl",
    help="å–å¾—ã—ãŸã„è¦ç´ ã®CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
)

# Page range
col1, col2 = st.sidebar.columns(2)
with col1:
    start_page = st.number_input("é–‹å§‹ãƒšãƒ¼ã‚¸", min_value=1, value=1)
with col2:
    end_page = st.number_input("çµ‚äº†ãƒšãƒ¼ã‚¸", min_value=1, value=5)

# Advanced settings
st.sidebar.markdown("### è©³ç´°è¨­å®š")
min_delay = st.sidebar.slider("æœ€å°å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰", 1, 10, 1)
max_delay = st.sidebar.slider("æœ€å¤§å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰", 1, 10, 5)
timeout = st.sidebar.slider("ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰", 5, 30, 10)

# CSS Selector Guide
st.markdown('<h2 class="section-header">ğŸ“– CSSã‚»ãƒ¬ã‚¯ã‚¿ã®å–å¾—æ–¹æ³•</h2>', unsafe_allow_html=True)

with st.expander("CSSã‚»ãƒ¬ã‚¯ã‚¿ã®å–å¾—æ‰‹é †", expanded=False):
    st.markdown("""
    ### æ‰‹é †ï¼š
    1. **å¯¾è±¡ã‚µã‚¤ãƒˆã‚’é–‹ã** - ãƒ–ãƒ©ã‚¦ã‚¶ã§å¯¾è±¡ã®Webãƒšãƒ¼ã‚¸ã‚’é–‹ãã¾ã™
    2. **è¦ç´ ã‚’å³ã‚¯ãƒªãƒƒã‚¯** - å–å¾—ã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆã‚„ãƒªãƒ³ã‚¯ã‚’å³ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™
    3. **æ¤œè¨¼ã‚’é¸æŠ** - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ã€Œæ¤œè¨¼ã€ã¾ãŸã¯ã€Œè¦ç´ ã‚’èª¿æŸ»ã€ã‚’é¸æŠ
    4. **è¦ç´ ã‚’ã‚³ãƒ”ãƒ¼** - é–‹ç™ºè€…ãƒ„ãƒ¼ãƒ«ã§è¦ç´ ã‚’å³ã‚¯ãƒªãƒƒã‚¯ â†’ Copy â†’ Copy selector
    5. **ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è²¼ã‚Šä»˜ã‘** - ä¸Šè¨˜ã®å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«è²¼ã‚Šä»˜ã‘ã¾ã™

    ### ã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹ã‚»ãƒ¬ã‚¯ã‚¿ã®ä¾‹ï¼š
    - `a` - ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯
    - `.class-name` - ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‚’æŒã¤è¦ç´ 
    - `#id-name` - ç‰¹å®šã®IDã‚’æŒã¤è¦ç´ 
    - `h1, h2, h3` - è¦‹å‡ºã—è¦ç´ 
    - `p` - æ®µè½è¦ç´ 
    - `div.container a` - containerã‚¯ãƒ©ã‚¹å†…ã®ãƒªãƒ³ã‚¯
    """)

# Function to scrape data
def scrape_data(base_url, css_selector, start_page, end_page, min_delay, max_delay, timeout):
    """
    Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    results = []
    progress_bar = st.progress(0)
    status_text = st.empty()

    total_pages = end_page - start_page + 1

    for page_num in range(start_page, end_page + 1):
        try:
            # Construct URL
            if '{}' in base_url:
                url = base_url.format(page_num)
            elif base_url.endswith('/'):
                url = f"{base_url}?page={page_num}"
            else:
                url = f"{base_url}&page={page_num}"

            status_text.text(f"ãƒšãƒ¼ã‚¸ {page_num} ã‚’å‡¦ç†ä¸­... URL: {url}")

            # Make request
            response = requests.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()

            # Parse HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            elements = soup.select(css_selector)

            # Extract data
            page_results = []
            for i, element in enumerate(elements):
                data = {
                    'page': page_num,
                    'index': i + 1,
                    'text': element.get_text(strip=True),
                    'href': element.get('href', ''),
                    'full_url': urljoin(url, element.get('href', '')) if element.get('href') else '',
                    'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                page_results.append(data)

            results.extend(page_results)

            # Update progress
            progress = (page_num - start_page + 1) / total_pages
            progress_bar.progress(progress)

            status_text.text(f"ãƒšãƒ¼ã‚¸ {page_num} å®Œäº† - {len(page_results)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")

            # Random delay
            if page_num < end_page:  # Don't delay after the last page
                delay = random.uniform(min_delay, max_delay)
                time.sleep(delay)

        except requests.exceptions.RequestException as e:
            st.error(f"ãƒšãƒ¼ã‚¸ {page_num} ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            continue
        except Exception as e:
            st.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ (ãƒšãƒ¼ã‚¸ {page_num}): {str(e)}")
            continue

    progress_bar.progress(1.0)
    status_text.text(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†! åˆè¨ˆ {len(results)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")

    return results

# Main content area
st.markdown('<h2 class="section-header">ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ</h2>', unsafe_allow_html=True)

# Validation
if st.button("ğŸ•·ï¸ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹", type="primary", use_container_width=True):
    if not base_url:
        st.error("ãƒ™ãƒ¼ã‚¹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    elif not css_selector:
        st.error("CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    elif start_page > end_page:
        st.error("é–‹å§‹ãƒšãƒ¼ã‚¸ã¯çµ‚äº†ãƒšãƒ¼ã‚¸ä»¥ä¸‹ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
    else:
        with st.spinner("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­..."):
            results = scrape_data(base_url, css_selector, start_page, end_page, min_delay, max_delay, timeout)

        if results:
            st.success(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†! {len(results)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")

            # Store results in session state
            st.session_state.scraping_results = results
            st.session_state.scraping_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

# Display results if available
if 'scraping_results' in st.session_state and st.session_state.scraping_results:
    st.markdown('<h2 class="section-header">ğŸ“Š å–å¾—ãƒ‡ãƒ¼ã‚¿</h2>', unsafe_allow_html=True)

    results = st.session_state.scraping_results
    df = pd.DataFrame(results)

    # Data preview
    st.markdown("### ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
    st.dataframe(df, use_container_width=True)

    # Statistics
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ç·ãƒ‡ãƒ¼ã‚¿æ•°", len(results))
    with col2:
        st.metric("ãƒšãƒ¼ã‚¸æ•°", df['page'].nunique())
    with col3:
        st.metric("ãƒªãƒ³ã‚¯æ•°", df[df['href'] != ''].shape[0])
    with col4:
        st.metric("ãƒ†ã‚­ã‚¹ãƒˆã®ã¿", df[df['href'] == ''].shape[0])

    # Download section
    st.markdown('<h2 class="section-header">ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰</h2>', unsafe_allow_html=True)

    timestamp = st.session_state.scraping_timestamp

    col1, col2, col3 = st.columns(3)

    with col1:
        # Excel download
        excel_buffer = io.BytesIO()
        df.to_excel(excel_buffer, index=False, engine='openpyxl')
        excel_buffer.seek(0)

        st.download_button(
            label="ğŸ“Š Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=excel_buffer.getvalue(),
            file_name=f"scraping_results_{timestamp}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )

    with col2:
        # CSV download
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')

        st.download_button(
            label="ğŸ“„ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_buffer.getvalue(),
            file_name=f"scraping_results_{timestamp}.csv",
            mime="text/csv",
            use_container_width=True
        )

    with col3:
        # Text download
        text_content = "\n".join([f"{item['text']} - {item['full_url']}" for item in results if item['text']])

        st.download_button(
            label="ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=text_content,
            file_name=f"scraping_results_{timestamp}.txt",
            mime="text/plain",
            use_container_width=True
        )

# Footer
st.markdown("---")
st.markdown("""
<div class="info-box">
<strong>ğŸ’¡ ä½¿ç”¨ä¸Šã®æ³¨æ„</strong><br>
â€¢ ã“ã®ãƒ„ãƒ¼ãƒ«ã¯æ•™è‚²ãƒ»ç ”ç©¶ç›®çš„ã§ã®ä½¿ç”¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™<br>
â€¢ å•†ç”¨åˆ©ç”¨ã®å ´åˆã¯ã€å¯¾è±¡ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ã‚’å¿…ãšç¢ºèªã—ã¦ãã ã•ã„<br>
â€¢ éåº¦ãªã‚¢ã‚¯ã‚»ã‚¹ã¯ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™<br>
â€¢ robots.txtãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’å°Šé‡ã—ã¦ãã ã•ã„
</div>
""", unsafe_allow_html=True)
