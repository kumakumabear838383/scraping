import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from urllib.parse import urljoin, urlparse
import re

def scrape_data(base_url, max_pages=5, delay_range=(1, 3)):
    """
    WalkerPlusã®ã‚¹ãƒãƒƒãƒˆæƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹é–¢æ•°

    Args:
        base_url (str): ãƒ™ãƒ¼ã‚¹URL
        max_pages (int): æœ€å¤§ãƒšãƒ¼ã‚¸æ•°
        delay_range (tuple): å¾…æ©Ÿæ™‚é–“ã®ç¯„å›²ï¼ˆç§’ï¼‰

    Returns:
        list: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
    """
    all_data = []

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¦User-Agentã‚’è¨­å®š
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    })

    for page_num in range(1, max_pages + 1):
        try:
            # URLç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ï¼ˆä¿®æ­£ç‰ˆï¼‰
            if page_num == 1:
                url = base_url.rstrip('/')  # 1ãƒšãƒ¼ã‚¸ç›®ã¯ç•ªå·ãªã—
            else:
                # ãƒ™ãƒ¼ã‚¹URLã‹ã‚‰æœ«å°¾ã®/ã‚’é™¤å»ã—ã¦ã€ãƒšãƒ¼ã‚¸ç•ªå·.htmlã‚’è¿½åŠ 
                clean_base_url = base_url.rstrip('/')
                url = f"{clean_base_url}/{page_num}.html"

            st.write(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page_num} ã‚’å‡¦ç†ä¸­: {url}")

            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
            response = session.get(url, timeout=10)
            response.raise_for_status()

            # BeautifulSoupã§HTMLã‚’è§£æ
            soup = BeautifulSoup(response.content, 'html.parser')

            # ã‚¹ãƒãƒƒãƒˆæƒ…å ±ã‚’æŠ½å‡ºï¼ˆWalkerPlusã®æ§‹é€ ã«åŸºã¥ãï¼‰
            spots = soup.find_all('div', class_='spot-item') or soup.find_all('li', class_='spot-list-item')

            if not spots:
                # åˆ¥ã®æ§‹é€ ã‚’è©¦ã™
                spots = soup.find_all('div', class_='item') or soup.find_all('article')

            if not spots:
                st.warning(f"ãƒšãƒ¼ã‚¸ {page_num} ã§ã‚¹ãƒãƒƒãƒˆæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                continue

            page_data = []
            for spot in spots:
                try:
                    # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
                    title_elem = (spot.find('h3') or 
                                spot.find('h2') or 
                                spot.find('a', class_='title') or
                                spot.find('div', class_='title'))
                    title = title_elem.get_text(strip=True) if title_elem else "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"

                    # ãƒªãƒ³ã‚¯æŠ½å‡º
                    link_elem = spot.find('a')
                    link = urljoin(url, link_elem['href']) if link_elem and link_elem.get('href') else ""

                    # ä½æ‰€æŠ½å‡º
                    address_elem = (spot.find('div', class_='address') or 
                                  spot.find('span', class_='address') or
                                  spot.find('p', class_='address'))
                    address = address_elem.get_text(strip=True) if address_elem else ""

                    # èª¬æ˜æŠ½å‡º
                    desc_elem = (spot.find('div', class_='description') or 
                               spot.find('p', class_='description') or
                               spot.find('div', class_='text'))
                    description = desc_elem.get_text(strip=True) if desc_elem else ""

                    # ã‚«ãƒ†ã‚´ãƒªæŠ½å‡º
                    category_elem = (spot.find('span', class_='category') or 
                                   spot.find('div', class_='category'))
                    category = category_elem.get_text(strip=True) if category_elem else ""

                    data = {
                        'ã‚¿ã‚¤ãƒˆãƒ«': title,
                        'ãƒªãƒ³ã‚¯': link,
                        'ä½æ‰€': address,
                        'èª¬æ˜': description,
                        'ã‚«ãƒ†ã‚´ãƒª': category,
                        'ãƒšãƒ¼ã‚¸': page_num,
                        'URL': url
                    }
                    page_data.append(data)

                except Exception as e:
                    st.warning(f"ã‚¹ãƒãƒƒãƒˆæƒ…å ±ã®æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            all_data.extend(page_data)
            st.success(f"âœ… ãƒšãƒ¼ã‚¸ {page_num}: {len(page_data)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")

            # æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸ã®å¾…æ©Ÿæ™‚é–“
            if page_num < max_pages:
                wait_time = random.uniform(delay_range[0], delay_range[1])
                st.info(f"â³ {wait_time:.1f}ç§’å¾…æ©Ÿä¸­...")
                time.sleep(wait_time)

        except requests.exceptions.RequestException as e:
            st.error(f"âŒ ãƒšãƒ¼ã‚¸ {page_num} ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            continue
        except Exception as e:
            st.error(f"âŒ ãƒšãƒ¼ã‚¸ {page_num} ã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    return all_data

def preview_page_structure(url):
    """
    ãƒšãƒ¼ã‚¸æ§‹é€ ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹é–¢æ•°
    """
    try:
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

        response = session.get(url, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # å¯èƒ½ãªã‚¹ãƒãƒƒãƒˆè¦ç´ ã‚’æ¢ã™
        possible_elements = [
            ('div.spot-item', soup.find_all('div', class_='spot-item')),
            ('li.spot-list-item', soup.find_all('li', class_='spot-list-item')),
            ('div.item', soup.find_all('div', class_='item')),
            ('article', soup.find_all('article')),
        ]

        st.write("### ğŸ” ãƒšãƒ¼ã‚¸æ§‹é€ åˆ†æ")
        for selector, elements in possible_elements:
            if elements:
                st.write(f"**{selector}**: {len(elements)} å€‹ã®è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

                # æœ€åˆã®è¦ç´ ã®æ§‹é€ ã‚’è¡¨ç¤º
                if elements:
                    first_element = elements[0]
                    st.write("æœ€åˆã®è¦ç´ ã®æ§‹é€ :")
                    st.code(str(first_element)[:500] + "..." if len(str(first_element)) > 500 else str(first_element))
                    break
        else:
            st.warning("ã‚¹ãƒãƒƒãƒˆè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

    except Exception as e:
        st.error(f"ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    st.title("ğŸ¢ WalkerPlus ã‚¹ãƒãƒƒãƒˆæƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«")
    st.markdown("---")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®š
    st.sidebar.header("âš™ï¸ è¨­å®š")

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆURLã‚’æ–°ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã«æ›´æ–°
    default_url = "https://www.walkerplus.com/spot_list/ar0623/sg0051/"
    base_url = st.sidebar.text_input(
        "ãƒ™ãƒ¼ã‚¹URL", 
        value=default_url,
        help="WalkerPlusã®ã‚¹ãƒãƒƒãƒˆä¸€è¦§ãƒšãƒ¼ã‚¸ã®URL"
    )

    max_pages = st.sidebar.slider("æœ€å¤§ãƒšãƒ¼ã‚¸æ•°", 1, 20, 5)

    delay_min = st.sidebar.slider("æœ€å°å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰", 0.5, 5.0, 1.0, 0.5)
    delay_max = st.sidebar.slider("æœ€å¤§å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰", 1.0, 10.0, 3.0, 0.5)

    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
    col1, col2 = st.columns([2, 1])

    with col1:
        if st.button("ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹", type="primary"):
            if not base_url:
                st.error("ãƒ™ãƒ¼ã‚¹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                return

            with st.spinner("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œä¸­..."):
                data = scrape_data(base_url, max_pages, (delay_min, delay_max))

            if data:
                st.success(f"ğŸ‰ åˆè¨ˆ {len(data)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸï¼")

                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
                df = pd.DataFrame(data)

                # ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
                st.subheader("ğŸ“Š å–å¾—ãƒ‡ãƒ¼ã‚¿")
                st.dataframe(df, use_container_width=True)

                # çµ±è¨ˆæƒ…å ±
                st.subheader("ğŸ“ˆ çµ±è¨ˆæƒ…å ±")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ç·ãƒ‡ãƒ¼ã‚¿æ•°", len(df))
                with col2:
                    st.metric("å‡¦ç†ãƒšãƒ¼ã‚¸æ•°", df['ãƒšãƒ¼ã‚¸'].nunique())
                with col3:
                    st.metric("å¹³å‡ãƒ‡ãƒ¼ã‚¿/ãƒšãƒ¼ã‚¸", f"{len(df)/df['ãƒšãƒ¼ã‚¸'].nunique():.1f}")

                # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                csv = df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv,
                    file_name=f"walkerplus_spots_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )

            else:
                st.error("âŒ ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

    with col2:
        if st.button("ğŸ” ãƒšãƒ¼ã‚¸æ§‹é€ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼"):
            if base_url:
                preview_page_structure(base_url)
            else:
                st.error("ãƒ™ãƒ¼ã‚¹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    # ä½¿ç”¨æ–¹æ³•
    with st.expander("ğŸ“– ä½¿ç”¨æ–¹æ³•"):
        st.markdown("""
        ### ğŸ¯ ã“ã®ãƒ„ãƒ¼ãƒ«ã®ä½¿ã„æ–¹

        1. **ãƒ™ãƒ¼ã‚¹URL**: WalkerPlusã®ã‚¹ãƒãƒƒãƒˆä¸€è¦§ãƒšãƒ¼ã‚¸ã®URLã‚’å…¥åŠ›
           - ä¾‹: `https://www.walkerplus.com/spot_list/ar0623/sg0051/`

        2. **æœ€å¤§ãƒšãƒ¼ã‚¸æ•°**: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°ã‚’è¨­å®š

        3. **å¾…æ©Ÿæ™‚é–“**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿæ™‚é–“ã‚’è¨­å®šï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ï¼‰

        4. **ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**: ãƒšãƒ¼ã‚¸æ§‹é€ ã‚’äº‹å‰ç¢ºèª

        5. **å®Ÿè¡Œ**: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹

        ### âš ï¸ æ³¨æ„äº‹é …
        - åˆ©ç”¨è¦ç´„ã‚’éµå®ˆã—ã¦ãã ã•ã„
        - éåº¦ãªã‚¢ã‚¯ã‚»ã‚¹ã¯é¿ã‘ã¦ãã ã•ã„
        - å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã®åˆ©ç”¨ã¯è‡ªå·±è²¬ä»»ã§è¡Œã£ã¦ãã ã•ã„

        ### ğŸ”§ URLæ§‹é€ ã«ã¤ã„ã¦
        - 1ãƒšãƒ¼ã‚¸ç›®: ãƒ™ãƒ¼ã‚¹URLï¼ˆç•ªå·ãªã—ï¼‰
        - 2ãƒšãƒ¼ã‚¸ç›®ä»¥é™: ãƒ™ãƒ¼ã‚¹URL + ãƒšãƒ¼ã‚¸ç•ªå·.html
        """)

if __name__ == "__main__":
    main()
